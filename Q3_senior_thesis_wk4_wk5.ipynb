{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6042148-53a9-44e9-8fef-64119cc4531d",
   "metadata": {},
   "source": [
    "# Reassemble Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8511a5ad-0794-4c08-a9b3-7ebda43176fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def reassemble_test_dataset(json_path, engl_paths, japn_paths):\n",
    "    \"\"\"\n",
    "    Reassembles the dataset from the original files using indices stored in a JSON file.\n",
    "\n",
    "    Args:\n",
    "    json_path (str): Path to the JSON file containing the indices.\n",
    "    engl_paths (list): List of paths to the English files in the order they were originally concatenated.\n",
    "    japn_paths (list): List of paths to the Japanese files in the order they were originally concatenated.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The reassembled DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the indices from the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        indices = json.load(json_file)\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Extract and concatenate the subsets using the indices\n",
    "    for engl_path, japn_path, index_key in zip(engl_paths, japn_paths, indices):\n",
    "        with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "            engl_lines = engl_file.readlines()\n",
    "            japn_lines = japn_file.readlines()\n",
    "        \n",
    "        # Extract the subset of lines using the index\n",
    "        start_index = indices[index_key]\n",
    "        engl_subset = engl_lines[start_index:start_index + 300]\n",
    "        japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "        # Create a new DataFrame with the extracted lines\n",
    "        new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Concatenate the new DataFrame with the existing one\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "json_path = './model_outputs/test/en_to_jp/index.json'\n",
    "engl_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.en',\n",
    "    './datasets/public/pheMT_final/tok.en',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en'\n",
    "]\n",
    "japn_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja',\n",
    "    './datasets/public/pheMT_final/tok.ja',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja'\n",
    "]\n",
    "kftt_phemt_aspec = reassemble_test_dataset(json_path, engl_paths, japn_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f4b3b9-bfc2-4da0-8846-6cf7b488ecb4",
   "metadata": {},
   "source": [
    "# M4T Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4f7fa4-a507-43c7-baa8-b5a89ef0c831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 24.3.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.3.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env\n",
      "\n",
      "  added / updated specs:\n",
      "    - libsndfile==1.0.31\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  gettext            conda-forge/osx-arm64::gettext-0.22.5-h8fbad5d_2 \n",
      "  gettext-tools      conda-forge/osx-arm64::gettext-tools-0.22.5-h8fbad5d_2 \n",
      "  libasprintf        conda-forge/osx-arm64::libasprintf-0.22.5-h8fbad5d_2 \n",
      "  libasprintf-devel  conda-forge/osx-arm64::libasprintf-devel-0.22.5-h8fbad5d_2 \n",
      "  libflac            conda-forge/osx-arm64::libflac-1.3.4-h07bb92c_0 \n",
      "  libgettextpo       conda-forge/osx-arm64::libgettextpo-0.22.5-h8fbad5d_2 \n",
      "  libgettextpo-devel conda-forge/osx-arm64::libgettextpo-devel-0.22.5-h8fbad5d_2 \n",
      "  libiconv           conda-forge/osx-arm64::libiconv-1.17-h0d3ecfb_2 \n",
      "  libintl            conda-forge/osx-arm64::libintl-0.22.5-h8fbad5d_2 \n",
      "  libintl-devel      conda-forge/osx-arm64::libintl-devel-0.22.5-h8fbad5d_2 \n",
      "  libogg             conda-forge/osx-arm64::libogg-1.3.4-h27ca646_1 \n",
      "  libopus            conda-forge/osx-arm64::libopus-1.3.1-h27ca646_1 \n",
      "  libsndfile         conda-forge/osx-arm64::libsndfile-1.0.31-h9f76cd9_1 \n",
      "  libvorbis          conda-forge/osx-arm64::libvorbis-1.3.7-h9f76cd9_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge libsndfile==1.0.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef5b1162-575a-4c78-897b-1922da356620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of seamlessM4T_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_v2. Set `force` to `True` to download again.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from seamless_communication.inference import Translator\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#translator = Translator(\"seamlessM4T_large\", \"vocoder_v2\", torch.device(\"mps:0\"), torch.float16)\n",
    "m4tv1_translator = Translator(\n",
    "\"seamlessM4T_large\",\n",
    "\"vocoder_v2\",\n",
    "device=torch.device(\"mps:0\"),\n",
    "dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "454de7c7-032b-43c9-88a5-b4938062ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def m4t_v1_translate_and_save(df, translator, src_lang, tgt_lang, output_file):\n",
    "    # src_lang, tgt_lang = 'eng' or 'jpn'\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                index_row = 'Japanese' if src_lang == 'jpn' else 'English'\n",
    "                source_text = row[index_row]\n",
    "\n",
    "                # Translate the text using the local translator\n",
    "                translated_text, _ = translator.predict(\n",
    "                    input=source_text,\n",
    "                    task_str=\"T2TT\",\n",
    "                    tgt_lang=tgt_lang,\n",
    "                    src_lang=src_lang,\n",
    "                    unit_generation_opts=None, \n",
    "                )\n",
    "                \n",
    "                # Write the translated text to the output file\n",
    "                file.write(str(translated_text[0]) + '\\n')\n",
    "                # print(\"SRC TEXT:\", source_text, \"\\n\", \"OUTPUT TEXT:\", str(translated_text[0]), \"\\n\") # TODO remove \n",
    "                # break # TODO REMOVE \n",
    "\n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2bf7b380-8c4e-4ff1-9be1-b479898738a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO eng->jap \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#m4t_v1_translate_and_save(df=kftt_phemt_aspec, translator=m4tv1_translator, src_lang='eng', tgt_lang='jpn', output_file='model_outputs/test/en_to_jp/m4tv1/out.txt')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# TODO jap->eng \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mm4t_v1_translate_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkftt_phemt_aspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm4tv1_translator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjpn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_outputs/test/jp_to_en/m4tv1/out.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 14\u001b[0m, in \u001b[0;36mm4t_v1_translate_and_save\u001b[0;34m(df, translator, src_lang, tgt_lang, output_file)\u001b[0m\n\u001b[1;32m     11\u001b[0m source_text \u001b[38;5;241m=\u001b[39m row[index_row]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Translate the text using the local translator\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m translated_text, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT2TT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit_generation_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Write the translated text to the output file\u001b[39;00m\n\u001b[1;32m     23\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(translated_text[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/seamless_communication/inference/translator.py:319\u001b[0m, in \u001b[0;36mTranslator.predict\u001b[0;34m(self, input, task_str, tgt_lang, src_lang, text_generation_opts, unit_generation_opts, spkr, sample_rate, unit_generation_ngram_filtering, duration_factor, prosody_encoder_input, src_text)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unit_generation_opts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     unit_generation_opts \u001b[38;5;241m=\u001b[39m SequenceGeneratorOptions(\n\u001b[1;32m    316\u001b[0m         beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, soft_max_seq_len\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    317\u001b[0m     )\n\u001b[0;32m--> 319\u001b[0m texts, units \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munit_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_modality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_modality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_generation_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit_generation_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit_generation_ngram_filtering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit_generation_ngram_filtering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduration_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprosody_encoder_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprosody_encoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_mintox \u001b[38;5;129;01mand\u001b[39;00m task_str \u001b[38;5;241m!=\u001b[39m Task\u001b[38;5;241m.\u001b[39mASR\u001b[38;5;241m.\u001b[39mname:\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mSPEECH:\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/seamless_communication/inference/translator.py:188\u001b[0m, in \u001b[0;36mTranslator.get_prediction\u001b[0;34m(cls, model, text_tokenizer, unit_tokenizer, seqs, padding_mask, input_modality, output_modality, tgt_lang, text_generation_opts, unit_generation_opts, unit_generation_ngram_filtering, duration_factor, prosody_encoder_input)\u001b[0m\n\u001b[1;32m    177\u001b[0m     unit_generation_opts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    179\u001b[0m generator \u001b[38;5;241m=\u001b[39m UnitYGenerator(\n\u001b[1;32m    180\u001b[0m     model,\n\u001b[1;32m    181\u001b[0m     text_tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     unit_opts\u001b[38;5;241m=\u001b[39munit_generation_opts,\n\u001b[1;32m    186\u001b[0m )\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_modality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_modality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngram_filtering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit_generation_ngram_filtering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduration_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprosody_encoder_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprosody_encoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/seamless_communication/inference/generator.py:269\u001b[0m, in \u001b[0;36mUnitYGenerator.__call__\u001b[0;34m(self, source_seqs, source_padding_mask, input_modality, output_modality, ngram_filtering, duration_factor, prosody_encoder_input)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt2t_converter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set `use_text_encoder` to `True` in your model config to encode text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         )\n\u001b[0;32m--> 269\u001b[0m     texts, text_gen_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt2t_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_padding_mask\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported input_modality: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_modality\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/text.py:152\u001b[0m, in \u001b[0;36mSequenceToTextConverter.batch_convert\u001b[0;34m(self, source_seqs, source_padding_mask)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(source_seqs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`source_seqs` must contain at least one element, but is empty instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/text.py:91\u001b[0m, in \u001b[0;36mSequenceToTextConverterBase._do_convert\u001b[0;34m(self, source_seqs, source_padding_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# (S) -> (N, S)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m target_prefix_seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_prefix_seq\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m generator_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_prefix_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m texts: List[StringLike] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, hypotheses \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generator_output\u001b[38;5;241m.\u001b[39mhypotheses):\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/beam_search.py:297\u001b[0m, in \u001b[0;36mBeamSearchSeq2SeqGenerator.__call__\u001b[0;34m(self, source_seqs, source_padding_mask, prompt_seqs, prompt_padding_mask)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`min_gen_len` must be less than or equal to `max_gen_len` (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_gen_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_gen_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead. Adjust your `max_gen_len` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m op \u001b[38;5;241m=\u001b[39m _BeamSearchSeq2SeqGeneratorOp(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    279\u001b[0m     encoder_output,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_hooks,\n\u001b[1;32m    295\u001b[0m )\n\u001b[0;32m--> 297\u001b[0m hypotheses \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Seq2SeqGeneratorOutput(hypotheses, encoder_output, encoder_padding_mask)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/beam_search.py:532\u001b[0m, in \u001b[0;36m_BeamSearchSequenceGeneratorOpBase.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_state()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_nr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_prompt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len):\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Sort the hypotheses by their scores before returning.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/beam_search.py:584\u001b[0m, in \u001b[0;36m_BeamSearchSequenceGeneratorOpBase._step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# Generate the next step output.\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_nr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_nr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_bag\u001b[38;5;241m.\u001b[39mincrement_step_nr()\n\u001b[1;32m    588\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/beam_search.py:917\u001b[0m, in \u001b[0;36m_BeamSearchSeq2SeqGeneratorOp._decode\u001b[0;34m(self, seqs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, seqs: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SequenceModelOutput:\n\u001b[0;32m--> 917\u001b[0m     decoder_output, decoder_padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We never use PAD in incremental decoding.\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_bag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mproject(decoder_output, decoder_padding_mask)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/seamless_communication/models/unity/model.py:246\u001b[0m, in \u001b[0;36mUnitYX2TModel.decode\u001b[0;34m(self, seqs, padding_mask, encoder_output, encoder_padding_mask, state_bag)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@finaloverride\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     state_bag: Optional[IncrementalStateBag] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[PaddingMask]]:\n\u001b[1;32m    242\u001b[0m     seqs, padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_frontend(\n\u001b[1;32m    243\u001b[0m         seqs, padding_mask, state_bag\u001b[38;5;241m=\u001b[39mstate_bag\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[no-any-return]\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_bag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/decoder.py:232\u001b[0m, in \u001b[0;36mStandardTransformerDecoder.forward\u001b[0;34m(self, seqs, padding_mask, encoder_output, encoder_padding_mask, state_bag)\u001b[0m\n\u001b[1;32m    227\u001b[0m     self_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_mask_factory(\n\u001b[1;32m    228\u001b[0m         seqs, keys\u001b[38;5;241m=\u001b[39mseqs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, state_bag\u001b[38;5;241m=\u001b[39mstate_bag\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mdrop_iter()):\n\u001b[0;32m--> 232\u001b[0m     seqs, padding_mask \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_bag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_output_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hook(layer_idx, seqs, padding_mask, num_layers):\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/decoder_layer.py:232\u001b[0m, in \u001b[0;36mStandardTransformerDecoderLayer.forward\u001b[0;34m(self, seqs, padding_mask, self_attn_mask, encoder_output, encoder_padding_mask, state_bag)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;129m@finaloverride\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     state_bag: Optional[IncrementalStateBag] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    231\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[PaddingMask]]:\n\u001b[0;32m--> 232\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_self_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_encoder_decoder_attn(\n\u001b[1;32m    235\u001b[0m         seqs, padding_mask, encoder_output, encoder_padding_mask, state_bag\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    238\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_ffn(seqs)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/decoder_layer.py:254\u001b[0m, in \u001b[0;36mStandardTransformerDecoderLayer._forward_self_attn\u001b[0;34m(self, seqs, padding_mask, self_attn_mask, state_bag)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_order \u001b[38;5;241m!=\u001b[39m TransformerNormOrder\u001b[38;5;241m.\u001b[39mPOST:\n\u001b[1;32m    252\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(seqs)\n\u001b[0;32m--> 254\u001b[0m seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_bag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_norm(seqs)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/multihead_attention.py:405\u001b[0m, in \u001b[0;36mStandardMultiheadAttention.forward\u001b[0;34m(self, seqs, padding_mask, keys, key_padding_mask, values, attn_mask, state_bag)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`key_padding_mask` must be `None` during incremental decoding.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# k: (N, S_step, M) -> (N, H_kv, S_step, K_h)\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# v: (N, S_step, M) -> (N, H_kv, S_step, V_h)\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_project_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m state \u001b[38;5;241m=\u001b[39m state_bag\u001b[38;5;241m.\u001b[39mget_state(\u001b[38;5;28mself\u001b[39m, AttentionState)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/multihead_attention.py:507\u001b[0m, in \u001b[0;36mStandardMultiheadAttention._project_kv\u001b[0;34m(self, keys, key_padding_mask, values, state_bag)\u001b[0m\n\u001b[1;32m    505\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(keys)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# (N, S, V) -> (N, S, V_proj)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# (N, S, K_proj) -> (N, H, S, K_h)\u001b[39;00m\n\u001b[1;32m    510\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/projection.py:130\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@finaloverride\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO eng->jap \n",
    "#m4t_v1_translate_and_save(df=kftt_phemt_aspec, translator=m4tv1_translator, src_lang='eng', tgt_lang='jpn', output_file='model_outputs/test/en_to_jp/m4tv1/out.txt')\n",
    "# TODO jap->eng \n",
    "m4t_v1_translate_and_save(df=kftt_phemt_aspec, translator=m4tv1_translator, src_lang='jpn', tgt_lang='eng', output_file='model_outputs/test/jp_to_en/m4tv1/out.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10ebc1-ff4e-40ec-b6dd-ece76a20065b",
   "metadata": {},
   "source": [
    "## M4T Benchmarking (TODO COME BACK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d079c42-5b8d-4b08-b32b-d0c39ea664c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7601caba-6c8d-427b-b759-29c296515bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1012af-e0f1-404d-80bf-09e322eef46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./env/lib/python3.10/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub in ./env/lib/python3.10/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./env/lib/python3.10/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./env/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./env/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.29.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a47728-055f-4404-99b2-63ef892fc839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.26.1-cp37-abi3-macosx_10_9_universal2.whl (404 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.0/404.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-5.26.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa480e-b237-4293-9afa-d259b486d3e5",
   "metadata": {},
   "source": [
    "# ALMA Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ef9337-b6a5-469b-a846-5def49a26b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10f887aa2224c1e8743bcdc16a0e6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish: I love machine translation.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load base model and LoRA weights\n",
    "model = AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-13B-R\", torch_dtype=torch.float16).to('mps')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/ALMA-13B-R\", padding_side='left')\n",
    "\n",
    "# Add the source sentence into the prompt template\n",
    "prompt=\"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=40, truncation=True).input_ids.to('mps')\n",
    "\n",
    "# Translation\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=20, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19cd1ddf-bcd8-4aa2-8f2c-a818956bf5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Jakugen developed a school later , so there were two schools of Shomyo in Ohara .\n",
      "\n",
      "THE PROMPT IS Translate this from English to Japanese:\n",
      "English: Jakugen developed a school later , so there were two schools of Shomyo in Ohara .\n",
      "\n",
      "Japanese:\n",
      "RAW OUTPUT Translate this from English to Japanese:\n",
      "English: Jakugen developed a school later , so there were two schools of Shomyo in Ohara .\n",
      "\n",
      "Japanese: 後に 八玄が学校を設立したため、大\n",
      "後に 八玄が学校を設立したため、大\n"
     ]
    }
   ],
   "source": [
    "test_english = kftt_phemt_aspec.loc[0]['English']\n",
    "print(\"Original:\", test_english) \n",
    "print(translate_text(alma_tokenizer, alma_model, test_english, 'English', 'Japanese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e70cad-7248-4e57-a581-6498861419de",
   "metadata": {},
   "source": [
    "### Actually do the inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a0e3b0-22ad-41e9-b5f7-6f694614e9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a2fa93ee5d44eba41731ce2325d5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED CREATING MODEL AND TOKENIZER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "alma_model = AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-13B-R\", torch_dtype=torch.float16).to('mps')\n",
    "alma_tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/ALMA-13B-R\", padding_side='left')\n",
    "print(\"FINISHED CREATING MODEL AND TOKENIZER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78953a11-7bf2-4d46-a470-f2959ad0acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(tokenizer, model, source_text, src_lang, tgt_lang):\n",
    "    # Create the prompt template based on the source and target languages\n",
    "    prompt = f\"Translate this from {src_lang} to {tgt_lang}:\\n{src_lang}: {source_text}\\n{tgt_lang}:\"\n",
    "    #print(f\"THE PROMPT IS {prompt}\") \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=200, truncation=True).input_ids.to('mps')\n",
    "\n",
    "    # Translation\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=100, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Extract the translated text from the outputs\n",
    "    # print(\"RAW OUTPUT\", outputs[0]) \n",
    "    translated_text = outputs[0].strip()\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "456eb12e-a5f8-432e-9fd2-90235417131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALMA_R_translate_and_save(df, model, tokenizer, src_lang, tgt_lang, output_file):\n",
    "    # src_lang, tgt_lang = 'English' or 'Japanese'\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            print(f\"OPENED FILE {output_file}\") \n",
    "            # Iterate over each row in the DataFrame\n",
    "            count = 0\n",
    "            for _, row in df.iterrows():\n",
    "                if count > 1: \n",
    "                    break \n",
    "                source_text = row[src_lang]\n",
    "\n",
    "                # Translate the text using the local translator\n",
    "                translated_text = translate_text(tokenizer, model, source_text, src_lang, tgt_lang) \n",
    "                \n",
    "                # Write the translated text to the output file\n",
    "                file.write(str(translated_text) + '\\n')\n",
    "                count +=1 \n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98b369-a281-45b5-afe7-1aa649e8beed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENED FILE model_outputs/test/jp_to_en/ALMA-R/out.txt\n"
     ]
    }
   ],
   "source": [
    "#ALMA_R_translate_and_save(df=kftt_phemt_aspec, model=alma_model, tokenizer=alma_tokenizer, src_lang='English', tgt_lang='Japanese', output_file='model_outputs/test/en_to_jp/ALMA-R/out2.txt')\n",
    "ALMA_R_translate_and_save(df=kftt_phemt_aspec, model=alma_model, tokenizer=alma_tokenizer, src_lang='Japanese', tgt_lang='English', output_file='model_outputs/test/jp_to_en/ALMA-R/out.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
