{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0b669-c8a8-4d2c-9eca-96ffa289294d",
   "metadata": {},
   "source": [
    "# Prepare Test set to be benchmarked across different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2efc4e43-56f1-4dc7-a92d-4fa9c3a82ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepl\n",
      "  Downloading deepl-1.17.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from deepl) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2023.11.17)\n",
      "Downloading deepl-1.17.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: deepl\n",
      "Successfully installed deepl-1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae5746-7f4c-48f9-9ba5-9104793eec2e",
   "metadata": {},
   "source": [
    "## Preprocess ASPEC to same format as other benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0b4367-7d83-451c-9af5-5e3735ef0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_aspec_data(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name of the input file (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Create the output file paths in the same directory as the input file\n",
    "    engl_output_path = os.path.join(input_dir, f\"{base_name}.raw.en\")\n",
    "    japn_output_path = os.path.join(input_dir, f\"{base_name}.raw.ja\")\n",
    "\n",
    "    # Open the input file for reading\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Open the output files for writing\n",
    "        with open(engl_output_path, 'w', encoding='utf-8') as engl_file, open(japn_output_path, 'w', encoding='utf-8') as japn_file:\n",
    "            # Iterate over each line in the input file\n",
    "            for line in file:\n",
    "                # Split the line into sections using the delimiter \" ||| \"\n",
    "                sections = line.strip().split(' ||| ')\n",
    "\n",
    "                # Extract the Japanese and English sections (last two sections)\n",
    "                japanese_text = sections[-2]\n",
    "                english_text = sections[-1]\n",
    "\n",
    "                # Write the Japanese text to the Japanese output file\n",
    "                japn_file.write(japanese_text + '\\n')\n",
    "\n",
    "                # Write the English text to the English output file\n",
    "                engl_file.write(english_text + '\\n')\n",
    "\n",
    "    print(f\"Data split completed. Output files: {engl_output_path} and {japn_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73d5e32-48aa-42d5-9b22-ffacd573f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en and ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja\n",
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.en and ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.ja\n"
     ]
    }
   ],
   "source": [
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.txt')\n",
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/test/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53975d6-75d7-40ff-92f9-4b30376aee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name and extension of the input file\n",
    "    base_name, ext = os.path.splitext(os.path.basename(file_path))\n",
    "\n",
    "    # Create the output file path in the same directory as the input file\n",
    "    output_path = os.path.join(input_dir, f\"{base_name}.tok{ext}\")\n",
    "\n",
    "    # Run KyTea on the input file and generate the tokenized output\n",
    "    subprocess.run(['kytea', '-notags', '-wsconst', 'D', '&lt', '-out', 'tok', '<', file_path, '>', output_path], check=True)\n",
    "\n",
    "    print(f\"Tokenization completed. Output file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac01dbdc-aeba-4b5b-93d4-e5a90786bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this function was really slow --> seconds become minutes, just did it in terminal instead kept it fast \n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en')\n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a2ab0-fa39-40bd-b944-cebda16c03a3",
   "metadata": {},
   "source": [
    "## Create massive dataframe with testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b518ecfb-ec7b-454a-850e-cb93a93e9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "random.seed(0) \n",
    "\n",
    "def create_parallel_dataframe(engl_path, japn_path, existing_df=None):\n",
    "    # Read the English and Japanese files\n",
    "    with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "        engl_lines = engl_file.readlines()\n",
    "        japn_lines = japn_file.readlines()\n",
    "\n",
    "    # Get the total number of lines in each file\n",
    "    total_lines = min(len(engl_lines), len(japn_lines))\n",
    "\n",
    "    # Check if there are at least 300 lines in both files\n",
    "    if total_lines < 300:\n",
    "        raise ValueError(\"Files must contain at least 300 lines.\")\n",
    "\n",
    "    # Generate a random starting index for the interval\n",
    "    start_index = random.randint(0, total_lines - 300)\n",
    "\n",
    "    # Extract the random interval of 300 lines from each file\n",
    "    engl_subset = engl_lines[start_index:start_index + 300]\n",
    "    japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "    # Create a new DataFrame with the extracted lines\n",
    "    new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # If an existing DataFrame is provided, concatenate the new data with it\n",
    "    if existing_df is not None:\n",
    "        df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        df = new_df\n",
    "\n",
    "    return df, start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf63cd-08de-43a5-9653-6795dc4b6e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56755041-cf4e-4335-b576-e9139b96d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               English  \\\n",
      "0    Jakugen developed a school later , so there we...   \n",
      "1      Subsequently , Shukai restored Ohara Shomyo .\\n   \n",
      "2    Tanchi established a stream based on a new for...   \n",
      "3    Since then , it became the center of Tendai Sh...   \n",
      "4    The Yuzunembutsu-shu sect , Jodo-shu sect and ...   \n",
      "..                                                 ...   \n",
      "895  Lignin   cresol   extracted   by   acetone   i...   \n",
      "896  A  fiber  mold   u sing   recycled   paper   i...   \n",
      "897  As  a   result ,  a  composite  with   high   ...   \n",
      "898  It   can   be separated into  compound  compos...   \n",
      "899  This  material   reports  the   result   of   ...   \n",
      "\n",
      "                                              Japanese  \n",
      "0    また 、 後 に 寂原 が 一派 を な し て 、 大原 に は 2 派 の 系統 の 声...  \n",
      "1                  のち に 宗快 が 大原 声明 を 再興 する に 至 っ た 。\\n  \n",
      "2             湛智 が 新し い 音楽 理論 に 基づ い た 流れ を 構築 し た 。\\n  \n",
      "3    以降 、 天台 声明 の 中枢 を なし 、 現在 の 天台 声明 に 継承 さ れ て い...  \n",
      "4    融通 念仏 宗 、 浄土 宗 、 浄土 真宗 の 声明 は 、 天台 声明 の 系統 で あ...  \n",
      "..                                                 ...  \n",
      "895  黒松 ， カエデ 材 の 脱脂 木紛 に ， ｐ クレゾール ， 硫酸 を 加え ， アセト...  \n",
      "896  再生 紙 使用 の ファイバ モールド を リグニン クレゾール の 溶液 で 浸漬 ， 溶...  \n",
      "897  その 結果 ， 複合 体 は 高 い 強度 特性 ， 優れ た 疎水 性 及び 寸法 安定 ...  \n",
      "898    なお 単純 な 溶媒 処理 で ， 複合 構成 素材 に 分離 ， 回収 可能 で あ る\\n  \n",
      "899  この 資料 は 平成 ５ 年 の 北海道 ， 東北 の 米穀類 の 冷害 に よ る 農業 ...  \n",
      "\n",
      "[900 rows x 2 columns] 223 516 285\n"
     ]
    }
   ],
   "source": [
    "kftt_out, kftt_start_index = create_parallel_dataframe('./datasets/public/kftt-data-1.0/data/tok/kyoto-test.en', './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja') \n",
    "kftt_and_phemt, phemt_start_index =  create_parallel_dataframe('./datasets/public/pheMT_final/tok.en', './datasets/public/pheMT_final/tok.ja', kftt_out)\n",
    "kftt_phemt_aspec, aspec_start_index = create_parallel_dataframe('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en', './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja', kftt_and_phemt)\n",
    "print(kftt_phemt_aspec, kftt_start_index, phemt_start_index, aspec_start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0fcd1-045b-499f-abae-b69a768c1c1b",
   "metadata": {},
   "source": [
    "# Do some checks on max characters so i dont go over free api limits and go bankrupt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3b4161e-7926-4456-ad1f-b7687bac4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_characters(df):\n",
    "    # Count the total number of characters in the Japanese column\n",
    "    total_japanese_chars = df['Japanese'].str.len().sum()\n",
    "\n",
    "    # Count the total number of characters in the English column\n",
    "    total_english_chars = df['English'].str.len().sum()\n",
    "\n",
    "    # Create a dictionary with the counts\n",
    "    char_counts = {\n",
    "        \"japanese_characters\": total_japanese_chars,\n",
    "        \"english_characters\": total_english_chars\n",
    "    }\n",
    "    print(char_counts) \n",
    "    return char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "835480ea-75ac-44a4-8b2a-519ecdff26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'japanese_characters': 56690, 'english_characters': 128381}\n",
      "Total characters going to API calls = 185071\n"
     ]
    }
   ],
   "source": [
    "tot_chars = count_total_characters(kftt_phemt_aspec.copy())\n",
    "total = tot_chars['japanese_characters'] + tot_chars['english_characters']\n",
    "print(f\"Total characters going to API calls = {total}\")\n",
    "assert total < 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5278baa-22ba-4cd5-aeda-2c7783650a9a",
   "metadata": {},
   "source": [
    "# API tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e5de-e70b-45d6-84f1-67b1e9d6e1e8",
   "metadata": {},
   "source": [
    "## deepL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c17095ef-8622-4486-af42-94f5ca7902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "def translate_and_save(df, api_key, src_language, output_file):\n",
    "    # src_langauge = 'English' or 'Japanese' \n",
    "    try:\n",
    "        # Initialize the DeepL translator\n",
    "        translator = deepl.Translator(api_key)\n",
    "\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                source_text = row[src_language]\n",
    "                target_language = 'EN-US' if src_language== 'Japanese' else 'JA'\n",
    "\n",
    "                # Translate the Japanese text to English\n",
    "                result = translator.translate_text(source_text, target_lang=target_language)\n",
    "                translated_text = result.text\n",
    "\n",
    "                # Write the translated text to the output file\n",
    "                file.write(translated_text + '\\n')\n",
    "\n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except deepl.exceptions.AuthorizationException:\n",
    "        print(\"Invalid API key. Please check your API key.\")\n",
    "\n",
    "    except deepl.exceptions.QuotaExceededException:\n",
    "        print(\"Quota exceeded. Please check your DeepL usage limits.\")\n",
    "\n",
    "    except deepl.exceptions.DeepLException as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a8a3c9-df3b-4034-9de9-ff80cb145944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            api_key = file.readline().strip()\n",
    "            return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error reading file: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54122365-caad-449a-a71a-6a1924f80549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_column(df, column, output_file):\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                japanese_text = row[column]\n",
    "\n",
    "                # Write the Japanese text to the output file\n",
    "                file.write(japanese_text + '\\n')\n",
    "\n",
    "        print(f\"{column} saved to: {output_file}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73ccb8c9-2d33-4b2e-80e7-198ecc317de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json_file(path, kftt_start_index, phe_mt_start_index, aspec_start_index):\n",
    "    # Create a dictionary with the provided indexes\n",
    "    data = {\n",
    "        \"kftt_start_index\": kftt_start_index,\n",
    "        \"phe_mt_start_index\": phe_mt_start_index,\n",
    "        \"aspec_start_index\": aspec_start_index\n",
    "    }\n",
    "    \n",
    "    # Write the dictionary to a file in JSON format\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dc3a599-012b-4c68-b339-31ef20e78bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in and out truth values for jp->en \n",
    "# save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/jp_to_en/in.txt')\n",
    "# save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/jp_to_en/out.txt')\n",
    "# write_json_file('model_outputs/test/jp_to_en/index.json', kftt_start_index, phemt_start_index, aspec_start_index)\n",
    "# Save in and out truth values for en->jp \n",
    "# save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/en_to_jp/in.txt')\n",
    "# save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/en_to_jp/out.txt')\n",
    "# write_json_file('model_outputs/test/en_to_jp/index.json', kftt_start_index, phemt_start_index, aspec_start_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a55544f4-bf31-4d2f-8b55-7455332df680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. Translations saved to: model_outputs/test/jp_to_en/deepL/out.txt\n",
      "Original Japanese: また 、 後 に 寂原 が 一派 を な し て 、 大原 に は 2 派 の 系統 の 声明 が あ っ た 。\n",
      "\n",
      "Translated English: None\n"
     ]
    }
   ],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "api_key = get_api_key('./datasets/private/apikey.txt')\n",
    "translated_english = translate_and_save(kftt_phemt_aspec, api_key, 'Japanese', 'model_outputs/test/jp_to_en/deepL/out.txt')\n",
    "print(\"Original Japanese:\", kftt_phemt_aspec['Japanese'].iloc[0])\n",
    "print(\"Translated English:\", translated_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbf31ee8-437f-41df-8d80-48dafd07df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "# api_key = get_api_key('./datasets/private/apikey.txt') \n",
    "# print(api_key) \n",
    "# print(kftt_phemt_aspec) \n",
    "# translated_japanese = translate_and_save(kftt_phemt_aspec, api_key, 'English', 'model_outputs/test/en_to_jp/deepL/out.txt') \n",
    "# print(\"Original English:\", kftt_phemt_aspec['English'].iloc[0])\n",
    "# print(\"Translated Japanese:\", translated_japanese)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de90a88-60e1-4eee-8299-cca6aa5724cc",
   "metadata": {},
   "source": [
    "## Google Translate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7888cde-2c4e-4050-a9fd-edd4dfa14f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
