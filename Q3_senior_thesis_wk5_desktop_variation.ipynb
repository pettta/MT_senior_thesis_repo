{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616d61c5-9817-4594-b720-1e405ae16f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def reassemble_test_dataset(json_path, engl_paths, japn_paths):\n",
    "    \"\"\"\n",
    "    Reassembles the dataset from the original files using indices stored in a JSON file.\n",
    "\n",
    "    Args:\n",
    "    json_path (str): Path to the JSON file containing the indices.\n",
    "    engl_paths (list): List of paths to the English files in the order they were originally concatenated.\n",
    "    japn_paths (list): List of paths to the Japanese files in the order they were originally concatenated.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The reassembled DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the indices from the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        indices = json.load(json_file)\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Extract and concatenate the subsets using the indices\n",
    "    for engl_path, japn_path, index_key in zip(engl_paths, japn_paths, indices):\n",
    "        with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "            engl_lines = engl_file.readlines()\n",
    "            japn_lines = japn_file.readlines()\n",
    "        \n",
    "        # Extract the subset of lines using the index\n",
    "        start_index = indices[index_key]\n",
    "        engl_subset = engl_lines[start_index:start_index + 300]\n",
    "        japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "        # Create a new DataFrame with the extracted lines\n",
    "        new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Concatenate the new DataFrame with the existing one\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "json_path = './model_outputs/test/en_to_jp/index.json'\n",
    "engl_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.en',\n",
    "    './datasets/public/pheMT_final/tok.en',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en'\n",
    "]\n",
    "japn_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja',\n",
    "    './datasets/public/pheMT_final/tok.ja',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja'\n",
    "]\n",
    "kftt_phemt_aspec = reassemble_test_dataset(json_path, engl_paths, japn_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d636dc24-84ee-4b66-8cbf-12a2d6ae207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/seamless_communication (from -r ./seamless_communication/demo/m4tv1/requirements.txt (line 2))\n",
      "  Cloning https://github.com/facebookresearch/seamless_communication to c:\\users\\karat\\appdata\\local\\temp\\pip-req-build-ojdketrm\n",
      "  Resolved https://github.com/facebookresearch/seamless_communication to commit 66054d4278e3ac792abc50dd0f27fe84b500d1e3\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fairseq2\n",
      "  Using cached fairseq2-0.2.1-py3-none-any.whl (191 kB)\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.29.0-py3-none-any.whl (12.3 MB)\n",
      "     ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.3 MB 640.0 kB/s eta 0:00:20\n",
      "     --------------------------------------- 0.0/12.3 MB 660.6 kB/s eta 0:00:19\n",
      "     --------------------------------------- 0.0/12.3 MB 660.6 kB/s eta 0:00:19\n",
      "     --------------------------------------- 0.1/12.3 MB 357.2 kB/s eta 0:00:35\n",
      "     --------------------------------------- 0.1/12.3 MB 492.8 kB/s eta 0:00:25\n",
      "     --------------------------------------- 0.1/12.3 MB 423.5 kB/s eta 0:00:29\n",
      "      -------------------------------------- 0.2/12.3 MB 623.6 kB/s eta 0:00:20\n",
      "      -------------------------------------- 0.3/12.3 MB 749.3 kB/s eta 0:00:17\n",
      "     - ------------------------------------- 0.3/12.3 MB 756.6 kB/s eta 0:00:16\n",
      "     - ------------------------------------- 0.5/12.3 MB 972.0 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.6/12.3 MB 1.1 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.7/12.3 MB 1.3 MB/s eta 0:00:09\n",
      "     --- ------------------------------------ 1.0/12.3 MB 1.6 MB/s eta 0:00:08\n",
      "     ---- ----------------------------------- 1.2/12.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.5/12.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ------ --------------------------------- 1.9/12.3 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.5/12.3 MB 3.2 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.1/12.3 MB 3.7 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.3/12.3 MB 3.9 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.1/12.3 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.7/12.3 MB 4.8 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.9/12.3 MB 4.9 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.3/12.3 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 8.9/12.3 MB 8.0 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.5/12.3 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.0/12.3 MB 15.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.3/12.3 MB 20.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\karat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r ./seamless_communication/demo/m4tv1/requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: torch in c:\\users\\karat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r ./seamless_communication/demo/m4tv1/requirements.txt (line 5)) (2.2.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\karat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from -r ./seamless_communication/demo/m4tv1/requirements.txt (line 6)) (2.2.1+cu118)\n",
      "Collecting jiwer~=3.0\n",
      "  Using cached jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: overrides~=7.3 in c:\\users\\karat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fairseq2->-r ./seamless_communication/demo/m4tv1/requirements.txt (line 1)) (7.7.0)\n",
      "Collecting numpy~=1.23\n",
      "  Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Requirement already satisfied: tqdm~=4.62 in c:\\users\\karat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fairseq2->-r ./seamless_communication/demo/m4tv1/requirements.txt (line 1)) (4.62.3)\n",
      "Collecting sacrebleu~=2.3\n",
      "  Using cached sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "Collecting torcheval~=0.0.6\n",
      "  Using cached torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
      "Collecting packaging~=23.1\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Collecting fairseq2\n",
      "  Using cached fairseq2-0.2.0-py3-none-any.whl (191 kB)\n",
      "  Downloading fairseq2-0.1.1-py3-none-any.whl (162 kB)\n",
      "     ---------------------------------------- 0.0/162.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 162.4/162.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pyyaml~=6.0 in c:\\users\\karat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from fairseq2->-r ./seamless_communication/demo/m4tv1/requirements.txt (line 1)) (6.0)\n",
      "  Downloading fairseq2-0.1.0-py3-none-any.whl (162 kB)\n",
      "     ---------------------------------------- 0.0/162.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 162.3/162.3 kB ? eta 0:00:00\n",
      "\n",
      "The conflict is caused by:\n",
      "    fairseq2 0.2.1 depends on fairseq2n==0.2.1\n",
      "    fairseq2 0.2.0 depends on fairseq2n==0.2.0\n",
      "    fairseq2 0.1.1 depends on fairseq2n==0.1.1\n",
      "    fairseq2 0.1.0 depends on fairseq2n==0.1.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/seamless_communication 'C:\\Users\\karat\\AppData\\Local\\Temp\\pip-req-build-ojdketrm'\n",
      "  Running command git submodule update --init --recursive -q\n",
      "ERROR: Cannot install -r ./seamless_communication/demo/m4tv1/requirements.txt (line 1) because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./seamless_communication/demo/m4tv1/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f71af91-1dbc-46eb-a00d-cee5550424f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003961801528930664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 6,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b430703f1ad43d498a721fce6adbb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003985404968261719,
       "initial": 314572800,
       "n": 314572800,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00001-of-00006.safetensors",
       "rate": null,
       "total": 4978265728,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf13d66632b4d5c96a5c846df6ffe32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   6%|6         | 315M/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0029909610748291016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00002-of-00006.safetensors",
       "rate": null,
       "total": 4970422160,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd2af1298894a2194365e1396542258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003989458084106445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00003-of-00006.safetensors",
       "rate": null,
       "total": 4970422184,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bab488b2bbe47e0bbd2f9dec6c042ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003991842269897461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00004-of-00006.safetensors",
       "rate": null,
       "total": 4933701432,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20d61364383464f852ee86cd45d4bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003989219665527344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00005-of-00006.safetensors",
       "rate": null,
       "total": 4933722144,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a786989a11014a1b809a45bde4e22e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002991914749145508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00006-of-00006.safetensors",
       "rate": null,
       "total": 1245236904,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634f3bb88ecd45ea9faf76d1f6f058b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003988981246948242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 6,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8603fae1ac4b719fa36e2b2a131260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 18.51 GiB is allocated by PyTorch, and 2.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m----> 5\u001b[0m alma_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhaoranxu/ALMA-13B-R\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      6\u001b[0m alma_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaoranxu/ALMA-13B-R\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINISHED CREATING MODEL AND TOKENIZER\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:2692\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2691\u001b[0m         )\n\u001b[1;32m-> 2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 18.51 GiB is allocated by PyTorch, and 2.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "alma_model = AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-13B-R\", torch_dtype=torch.float16).to('cuda') \n",
    "alma_tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/ALMA-13B-R\", padding_side='left')\n",
    "print(\"FINISHED CREATING MODEL AND TOKENIZER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf74d75-af7e-4149-835e-e0e1fea5bd99",
   "metadata": {},
   "source": [
    "## OK So not enough VRAM on 4070TI for inference with this, lets try smaller verison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266b6cb8-f0ef-4138-85a9-22737784d6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "     ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/991.5 kB 660.6 kB/s eta 0:00:02\n",
      "     - ----------------------------------- 30.7/991.5 kB 660.6 kB/s eta 0:00:02\n",
      "     - ----------------------------------- 30.7/991.5 kB 660.6 kB/s eta 0:00:02\n",
      "     --- --------------------------------- 81.9/991.5 kB 416.7 kB/s eta 0:00:03\n",
      "     ---- ------------------------------- 112.6/991.5 kB 544.7 kB/s eta 0:00:02\n",
      "     ---- ------------------------------- 122.9/991.5 kB 423.5 kB/s eta 0:00:03\n",
      "     ------- ---------------------------- 194.6/991.5 kB 620.6 kB/s eta 0:00:02\n",
      "     --------- -------------------------- 256.0/991.5 kB 714.4 kB/s eta 0:00:02\n",
      "     ---------- ------------------------- 286.7/991.5 kB 681.0 kB/s eta 0:00:02\n",
      "     --------------- -------------------- 440.3/991.5 kB 949.4 kB/s eta 0:00:01\n",
      "     --------------------- ---------------- 573.4/991.5 kB 1.2 MB/s eta 0:00:01\n",
      "     ----------------------- -------------- 604.2/991.5 kB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 768.0/991.5 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 931.8/991.5 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  983.0/991.5 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 991.5/991.5 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34f61903-a957-4520-a893-68aded1f2afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006981372833251953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5525676a32b74715a9fee169dcc43a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED CREATING MODEL AND TOKENIZER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "alma_model = AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-7B-R\", torch_dtype=torch.float16).to('cuda') \n",
    "alma_tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/ALMA-7B-R\", padding_side='left')\n",
    "print(\"FINISHED CREATING MODEL AND TOKENIZER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c51b15a-2adc-4a73-a9d6-dfb67161e4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish: I love machine translation.']\n"
     ]
    }
   ],
   "source": [
    "# Add the source sentence into the prompt template\n",
    "prompt=\"Translate this from Chinese to English:\\nChinese: 我爱机器翻译。\\nEnglish:\"\n",
    "input_ids = alma_tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=200, truncation=True).input_ids.cuda()\n",
    "\n",
    "# Translation\n",
    "with torch.no_grad():\n",
    "    generated_ids =alma_model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=200, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "outputs = alma_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86f6b4ef-fbde-4a75-a542-1913d940e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(tokenizer, model, source_text, src_lang, tgt_lang):\n",
    "    # Create the prompt template based on the source and target languages\n",
    "    prompt = f\"Translate this from {src_lang} to {tgt_lang}:\\n{src_lang}: {source_text}\\n{tgt_lang}:\"\n",
    "    #print(f\"THE PROMPT IS {prompt}\") \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=200, truncation=True).input_ids.cuda()\n",
    "\n",
    "    # Translation\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=200, do_sample=True, temperature=0.6, top_p=0.9)\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Extract the translated text from the outputs\n",
    "    #print(\"RAW OUTPUT\", outputs[0]) \n",
    "    translated_text = outputs[0].split(f\"{tgt_lang}:\")[-1].strip()\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "812526f1-c968-49c4-bae6-fd550fa4fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALMA_R_translate_and_save(df, model, tokenizer, src_lang, tgt_lang, output_file):\n",
    "    # src_lang, tgt_lang = 'English' or 'Japanese'\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            print(f\"OPENED FILE {output_file}\") \n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows(): \n",
    "                source_text = row[src_lang]\n",
    "\n",
    "                # Translate the text using the local translator\n",
    "                translated_text = translate_text(tokenizer, model, source_text, src_lang, tgt_lang) \n",
    "                \n",
    "                # Write the translated text to the output file\n",
    "                file.write(str(translated_text) + '\\n\\n') \n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3af402ad-50cd-41c2-a404-57a8274eadf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENED FILE model_outputs/test/jp_to_en/ALMA-R/out3.txt\n",
      "Translation completed. Translations saved to: model_outputs/test/jp_to_en/ALMA-R/out3.txt\n"
     ]
    }
   ],
   "source": [
    "#ALMA_R_translate_and_save(df=kftt_phemt_aspec, model=alma_model, tokenizer=alma_tokenizer, src_lang='English', tgt_lang='Japanese', output_file='model_outputs/test/en_to_jp/ALMA-R/out3.txt')\n",
    "ALMA_R_translate_and_save(df=kftt_phemt_aspec, model=alma_model, tokenizer=alma_tokenizer, src_lang='Japanese', tgt_lang='English', output_file='model_outputs/test/jp_to_en/ALMA-R/out3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb4c49f-91e3-4daf-9076-fcba55197f5c",
   "metadata": {},
   "source": [
    "## Ok so engl to japn direction doesnt work on 7b but it does on 13b "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
