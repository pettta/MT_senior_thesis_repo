{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0b669-c8a8-4d2c-9eca-96ffa289294d",
   "metadata": {},
   "source": [
    "# Prepare Test set to be benchmarked across different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2efc4e43-56f1-4dc7-a92d-4fa9c3a82ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepl\n",
      "  Downloading deepl-1.17.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from deepl) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2023.11.17)\n",
      "Downloading deepl-1.17.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: deepl\n",
      "Successfully installed deepl-1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae5746-7f4c-48f9-9ba5-9104793eec2e",
   "metadata": {},
   "source": [
    "## Preprocess ASPEC to same format as other benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0b4367-7d83-451c-9af5-5e3735ef0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_aspec_data(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name of the input file (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Create the output file paths in the same directory as the input file\n",
    "    engl_output_path = os.path.join(input_dir, f\"{base_name}.raw.en\")\n",
    "    japn_output_path = os.path.join(input_dir, f\"{base_name}.raw.ja\")\n",
    "\n",
    "    # Open the input file for reading\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Open the output files for writing\n",
    "        with open(engl_output_path, 'w', encoding='utf-8') as engl_file, open(japn_output_path, 'w', encoding='utf-8') as japn_file:\n",
    "            # Iterate over each line in the input file\n",
    "            for line in file:\n",
    "                # Split the line into sections using the delimiter \" ||| \"\n",
    "                sections = line.strip().split(' ||| ')\n",
    "\n",
    "                # Extract the Japanese and English sections (last two sections)\n",
    "                japanese_text = sections[-2]\n",
    "                english_text = sections[-1]\n",
    "\n",
    "                # Write the Japanese text to the Japanese output file\n",
    "                japn_file.write(japanese_text + '\\n')\n",
    "\n",
    "                # Write the English text to the English output file\n",
    "                engl_file.write(english_text + '\\n')\n",
    "\n",
    "    print(f\"Data split completed. Output files: {engl_output_path} and {japn_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73d5e32-48aa-42d5-9b22-ffacd573f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en and ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja\n",
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.en and ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.ja\n"
     ]
    }
   ],
   "source": [
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.txt')\n",
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/test/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53975d6-75d7-40ff-92f9-4b30376aee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name and extension of the input file\n",
    "    base_name, ext = os.path.splitext(os.path.basename(file_path))\n",
    "\n",
    "    # Create the output file path in the same directory as the input file\n",
    "    output_path = os.path.join(input_dir, f\"{base_name}.tok{ext}\")\n",
    "\n",
    "    # Run KyTea on the input file and generate the tokenized output\n",
    "    subprocess.run(['kytea', '-notags', '-wsconst', 'D', '&lt', '-out', 'tok', '<', file_path, '>', output_path], check=True)\n",
    "\n",
    "    print(f\"Tokenization completed. Output file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac01dbdc-aeba-4b5b-93d4-e5a90786bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this function was really slow --> seconds become minutes, just did it in terminal instead kept it fast \n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en')\n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a2ab0-fa39-40bd-b944-cebda16c03a3",
   "metadata": {},
   "source": [
    "## Create massive dataframe with testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b518ecfb-ec7b-454a-850e-cb93a93e9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "random.seed(0) \n",
    "\n",
    "def create_parallel_dataframe(engl_path, japn_path, existing_df=None):\n",
    "    # Read the English and Japanese files\n",
    "    with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "        engl_lines = engl_file.readlines()\n",
    "        japn_lines = japn_file.readlines()\n",
    "\n",
    "    # Get the total number of lines in each file\n",
    "    total_lines = min(len(engl_lines), len(japn_lines))\n",
    "\n",
    "    # Check if there are at least 300 lines in both files\n",
    "    if total_lines < 300:\n",
    "        raise ValueError(\"Files must contain at least 300 lines.\")\n",
    "\n",
    "    # Generate a random starting index for the interval\n",
    "    start_index = random.randint(0, total_lines - 300)\n",
    "\n",
    "    # Extract the random interval of 300 lines from each file\n",
    "    engl_subset = engl_lines[start_index:start_index + 300]\n",
    "    japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "    # Create a new DataFrame with the extracted lines\n",
    "    new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # If an existing DataFrame is provided, concatenate the new data with it\n",
    "    if existing_df is not None:\n",
    "        df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        df = new_df\n",
    "\n",
    "    return df, start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf63cd-08de-43a5-9653-6795dc4b6e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56755041-cf4e-4335-b576-e9139b96d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               English  \\\n",
      "0    Jakugen developed a school later , so there we...   \n",
      "1      Subsequently , Shukai restored Ohara Shomyo .\\n   \n",
      "2    Tanchi established a stream based on a new for...   \n",
      "3    Since then , it became the center of Tendai Sh...   \n",
      "4    The Yuzunembutsu-shu sect , Jodo-shu sect and ...   \n",
      "..                                                 ...   \n",
      "895  Lignin   cresol   extracted   by   acetone   i...   \n",
      "896  A  fiber  mold   u sing   recycled   paper   i...   \n",
      "897  As  a   result ,  a  composite  with   high   ...   \n",
      "898  It   can   be separated into  compound  compos...   \n",
      "899  This  material   reports  the   result   of   ...   \n",
      "\n",
      "                                              Japanese  \n",
      "0    また 、 後 に 寂原 が 一派 を な し て 、 大原 に は 2 派 の 系統 の 声...  \n",
      "1                  のち に 宗快 が 大原 声明 を 再興 する に 至 っ た 。\\n  \n",
      "2             湛智 が 新し い 音楽 理論 に 基づ い た 流れ を 構築 し た 。\\n  \n",
      "3    以降 、 天台 声明 の 中枢 を なし 、 現在 の 天台 声明 に 継承 さ れ て い...  \n",
      "4    融通 念仏 宗 、 浄土 宗 、 浄土 真宗 の 声明 は 、 天台 声明 の 系統 で あ...  \n",
      "..                                                 ...  \n",
      "895  黒松 ， カエデ 材 の 脱脂 木紛 に ， ｐ クレゾール ， 硫酸 を 加え ， アセト...  \n",
      "896  再生 紙 使用 の ファイバ モールド を リグニン クレゾール の 溶液 で 浸漬 ， 溶...  \n",
      "897  その 結果 ， 複合 体 は 高 い 強度 特性 ， 優れ た 疎水 性 及び 寸法 安定 ...  \n",
      "898    なお 単純 な 溶媒 処理 で ， 複合 構成 素材 に 分離 ， 回収 可能 で あ る\\n  \n",
      "899  この 資料 は 平成 ５ 年 の 北海道 ， 東北 の 米穀類 の 冷害 に よ る 農業 ...  \n",
      "\n",
      "[900 rows x 2 columns] 223 516 285\n"
     ]
    }
   ],
   "source": [
    "kftt_out, kftt_start_index = create_parallel_dataframe('./datasets/public/kftt-data-1.0/data/tok/kyoto-test.en', './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja') \n",
    "kftt_and_phemt, phemt_start_index =  create_parallel_dataframe('./datasets/public/pheMT_final/tok.en', './datasets/public/pheMT_final/tok.ja', kftt_out)\n",
    "kftt_phemt_aspec, aspec_start_index = create_parallel_dataframe('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en', './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja', kftt_and_phemt)\n",
    "print(kftt_phemt_aspec, kftt_start_index, phemt_start_index, aspec_start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0fcd1-045b-499f-abae-b69a768c1c1b",
   "metadata": {},
   "source": [
    "# Do some checks on max characters so i dont go over free api limits and go bankrupt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3b4161e-7926-4456-ad1f-b7687bac4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_characters(df):\n",
    "    # Count the total number of characters in the Japanese column\n",
    "    total_japanese_chars = df['Japanese'].str.len().sum()\n",
    "\n",
    "    # Count the total number of characters in the English column\n",
    "    total_english_chars = df['English'].str.len().sum()\n",
    "\n",
    "    # Create a dictionary with the counts\n",
    "    char_counts = {\n",
    "        \"japanese_characters\": total_japanese_chars,\n",
    "        \"english_characters\": total_english_chars\n",
    "    }\n",
    "    print(char_counts) \n",
    "    return char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "835480ea-75ac-44a4-8b2a-519ecdff26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'japanese_characters': 56690, 'english_characters': 128381}\n",
      "Total characters going to API calls = 185071\n"
     ]
    }
   ],
   "source": [
    "tot_chars = count_total_characters(kftt_phemt_aspec.copy())\n",
    "total = tot_chars['japanese_characters'] + tot_chars['english_characters']\n",
    "print(f\"Total characters going to API calls = {total}\")\n",
    "assert total < 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5278baa-22ba-4cd5-aeda-2c7783650a9a",
   "metadata": {},
   "source": [
    "# API tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e5de-e70b-45d6-84f1-67b1e9d6e1e8",
   "metadata": {},
   "source": [
    "## deepL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17095ef-8622-4486-af42-94f5ca7902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "def translate_and_save(df, api_key, src_language, output_file):\n",
    "    # src_langauge = 'English' or 'Japanese' \n",
    "    try:\n",
    "        # Initialize the DeepL translator\n",
    "        translator = deepl.Translator(api_key)\n",
    "\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                source_text = row[src_language]\n",
    "                target_language = 'EN-US' if src_language== 'Japanese' else 'JA'\n",
    "\n",
    "                # Translate the Japanese text to English\n",
    "                result = translator.translate_text(source_text, target_lang=target_language)\n",
    "                translated_text = result.text\n",
    "\n",
    "                # Write the translated text to the output file\n",
    "                file.write(translated_text + '\\n')\n",
    "\n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except deepl.exceptions.AuthorizationException:\n",
    "        print(\"Invalid API key. Please check your API key.\")\n",
    "\n",
    "    except deepl.exceptions.QuotaExceededException:\n",
    "        print(\"Quota exceeded. Please check your DeepL usage limits.\")\n",
    "\n",
    "    except deepl.exceptions.DeepLException as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a8a3c9-df3b-4034-9de9-ff80cb145944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            api_key = file.readline().strip()\n",
    "            return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error reading file: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54122365-caad-449a-a71a-6a1924f80549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_column(df, column, output_file):\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                japanese_text = row[column]\n",
    "\n",
    "                # Write the Japanese text to the output file\n",
    "                file.write(japanese_text + '\\n')\n",
    "\n",
    "        print(f\"{column} saved to: {output_file}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73ccb8c9-2d33-4b2e-80e7-198ecc317de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json_file(path, kftt_start_index, phe_mt_start_index, aspec_start_index):\n",
    "    # Create a dictionary with the provided indexes\n",
    "    data = {\n",
    "        \"kftt_start_index\": kftt_start_index,\n",
    "        \"phe_mt_start_index\": phe_mt_start_index,\n",
    "        \"aspec_start_index\": aspec_start_index\n",
    "    }\n",
    "    \n",
    "    # Write the dictionary to a file in JSON format\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dc3a599-012b-4c68-b339-31ef20e78bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in and out truth values for jp->en \n",
    "# save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/jp_to_en/in.txt')\n",
    "# save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/jp_to_en/out.txt')\n",
    "# write_json_file('model_outputs/test/jp_to_en/index.json', kftt_start_index, phemt_start_index, aspec_start_index)\n",
    "# Save in and out truth values for en->jp \n",
    "# save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/en_to_jp/in.txt')\n",
    "# save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/en_to_jp/out.txt')\n",
    "# write_json_file('model_outputs/test/en_to_jp/index.json', kftt_start_index, phemt_start_index, aspec_start_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a55544f4-bf31-4d2f-8b55-7455332df680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "#api_key = get_api_key('./datasets/private/deepl_apikey.txt')\n",
    "# translated_english = translate_and_save(kftt_phemt_aspec, api_key, 'Japanese', 'model_outputs/test/jp_to_en/deepL/out.txt')\n",
    "# print(\"Original Japanese:\", kftt_phemt_aspec['Japanese'].iloc[0])\n",
    "# print(\"Translated English:\", translated_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbf31ee8-437f-41df-8d80-48dafd07df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "# api_key = get_api_key('./datasets/private/apikey.txt') \n",
    "# print(api_key) \n",
    "# print(kftt_phemt_aspec) \n",
    "# translated_japanese = translate_and_save(kftt_phemt_aspec, api_key, 'English', 'model_outputs/test/en_to_jp/deepL/out.txt') \n",
    "# print(\"Original English:\", kftt_phemt_aspec['English'].iloc[0])\n",
    "# print(\"Translated Japanese:\", translated_japanese)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b689d22-1781-4274-8fef-bf9fcc918b39",
   "metadata": {},
   "source": [
    "## Evaluation Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "691bfbda-984a-49e5-9d14-0cee81ff1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu==2.0.0\n",
      "  Obtaining dependency information for sacrebleu==2.0.0 from https://files.pythonhosted.org/packages/fa/63/b3c11f951eafa2dc296862431f29fb12dbe191cb72217cf88ed04c32086b/sacrebleu-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached sacrebleu-2.0.0-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: portalocker in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (2.8.2)\n",
      "Requirement already satisfied: regex in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (2024.4.16)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (1.26.4)\n",
      "Requirement already satisfied: colorama in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (0.4.6)\n",
      "Using cached sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "Installing collected packages: sacrebleu\n",
      "Successfully installed sacrebleu-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sacrebleu==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cc5434d-abd0-4cfe-885f-a9e2c2541f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu[ja] in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: portalocker in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (2.8.2)\n",
      "Requirement already satisfied: regex in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (1.26.2)\n",
      "Requirement already satisfied: colorama in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (0.4.6)\n",
      "Requirement already satisfied: lxml in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (5.2.1)\n",
      "Collecting mecab-python3<=1.0.6,>=1.0.5 (from sacrebleu[ja])\n",
      "  Downloading mecab-python3-1.0.6.tar.gz (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ipadic<2.0,>=1.0 (from sacrebleu[ja])\n",
      "  Using cached ipadic-1.0.0-py3-none-any.whl\n",
      "Building wheels for collected packages: mecab-python3\n",
      "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[21 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages/setuptools/__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001b[31m   \u001b[0m WARNING setuptools_scm.pyproject_reading toml section missing 'pyproject.toml does not contain a tool.setuptools_scm section'\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-310/MeCab\n",
      "  \u001b[31m   \u001b[0m copying src/MeCab/__init__.py -> build/lib.macosx-11.0-arm64-cpython-310/MeCab\n",
      "  \u001b[31m   \u001b[0m copying src/MeCab/cli.py -> build/lib.macosx-11.0-arm64-cpython-310/MeCab\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: 'mecab-config'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for mecab-python3\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for mecab-python3\n",
      "Failed to build mecab-python3\n",
      "\u001b[31mERROR: Could not build wheels for mecab-python3, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"sacrebleu[ja]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22cd6041-f8b8-49a5-bb65-8454c22e6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF \n",
    "def evaluate_translation(input_file, translated_file, reference_file):\n",
    "    # Function to read a file and extract non-blank lines\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "        return lines\n",
    "\n",
    "    # Read the files\n",
    "    input_lines = read_file(input_file)\n",
    "    translated_lines = read_file(translated_file)\n",
    "    reference_lines = [read_file(reference_file)]  # Note the list wrapping for multiple references support\n",
    "    print(translated_lines[1]) # TODO remove\n",
    "    print(reference_lines[0][1]) # TODO remove\n",
    "\n",
    "    # Initialize the BLEU object\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF() \n",
    "\n",
    "    # Compute the BLEU score\n",
    "    score = bleu.corpus_score(translated_lines, reference_lines)\n",
    "    score2 = chrf.corpus_score(translated_lines, reference_lines)\n",
    "\n",
    "    # Print and return the BLEU score and its detailed breakdown\n",
    "    print(f\"Bleu Score: {score.score}\")\n",
    "    print(f\"CHRF Score: {score2.score}\")\n",
    "    print(f\"Full report 1: {score}\")\n",
    "    print(f\"Full report 2: {score2}\")\n",
    "    return (score.score, score2.score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc059297-4954-4dcd-a9d3-6bd079e543ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Later, Soukai revived the Ohara-voice.\n",
      "Subsequently , Shukai restored Ohara Shomyo .\n",
      "Bleu Score: 17.431383539831966\n",
      "CHRF Score: 48.86399062729437\n",
      "Full report 1: BLEU = 17.43 52.3/24.3/13.2/7.7 (BP = 0.920 ratio = 0.923 hyp_len = 18989 ref_len = 20572)\n",
      "Full report 2: chrF2 = 48.86\n",
      "その後、秀海は大原照明を復活させた。\n",
      "のち に 宗快 が 大原 声明 を 再興 する に 至 っ た 。\n",
      "Bleu Score: 0.0001015339034000896\n",
      "CHRF Score: 35.04544963210478\n",
      "Full report 1: BLEU = 0.00 12.0/10.2/3.7/3.3 (BP = 0.000 ratio = 0.083 hyp_len = 1844 ref_len = 22163)\n",
      "Full report 2: chrF2 = 35.05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0001015339034000896"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_translation(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/deepL/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "evaluate_translation(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/deepL/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de90a88-60e1-4eee-8299-cca6aa5724cc",
   "metadata": {},
   "source": [
    "## Google Translate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7888cde-2c4e-4050-a9fd-edd4dfa14f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate_v2\n",
    "import pandas as pd\n",
    "\n",
    "def google_translate_and_save(df, project, api_key, src_language, output_file):\n",
    "    # src_language = 'English' or 'Japanese'\n",
    "    try:\n",
    "        # Initialize the Google Translate client\n",
    "        #project=project, credentials=api_key TODO remove \n",
    "        client = translate_v2.Client(client_options={\"api_key\": api_key})\n",
    "        row = df.iloc[0] # TODO REMOVE \n",
    "        src_text = row[src_language] # TODO REMOVE\n",
    "        target_language = 'en-US' if src_language == 'Japanese' else 'ja-JP' # TODO REMOVE \n",
    "        print(client.translate(src_text, target_language=target_language)) # TODO REMOVE  \n",
    "        # Open the output file in write mode\n",
    "        # with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        #     # Iterate over each row in the DataFrame\n",
    "        #     for _, row in df.iterrows():\n",
    "        #         source_text = row[src_language]\n",
    "        #         target_language = 'en-US' if src_language == 'Japanese' else 'ja-JP'\n",
    "\n",
    "        #         # Translate the text\n",
    "        #         result = client.translate(source_text, target_language=target_language)\n",
    "        #         translated_text = result['translatedText']\n",
    "\n",
    "        #         # Write the translated text to the output file\n",
    "        #         file.write(translated_text + '\\n')\n",
    "\n",
    "        # print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46881f15-0bff-46b3-8697-e448a678c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def get_google_project_and_credentials(file_path):\n",
    "    try:\n",
    "        # Open the JSON file for reading\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Load data from JSON file into a Python dictionary\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Return the dictionary containing the API key and project information\n",
    "        return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} does not exist.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: The file is not a valid JSON.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc1566f2-d7d3-47a5-9411-31a809a1c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def reassemble_test_dataset(json_path, engl_paths, japn_paths):\n",
    "    \"\"\"\n",
    "    Reassembles the dataset from the original files using indices stored in a JSON file.\n",
    "\n",
    "    Args:\n",
    "    json_path (str): Path to the JSON file containing the indices.\n",
    "    engl_paths (list): List of paths to the English files in the order they were originally concatenated.\n",
    "    japn_paths (list): List of paths to the Japanese files in the order they were originally concatenated.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The reassembled DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the indices from the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        indices = json.load(json_file)\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Extract and concatenate the subsets using the indices\n",
    "    for engl_path, japn_path, index_key in zip(engl_paths, japn_paths, indices):\n",
    "        with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "            engl_lines = engl_file.readlines()\n",
    "            japn_lines = japn_file.readlines()\n",
    "        \n",
    "        # Extract the subset of lines using the index\n",
    "        start_index = indices[index_key]\n",
    "        engl_subset = engl_lines[start_index:start_index + 300]\n",
    "        japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "        # Create a new DataFrame with the extracted lines\n",
    "        new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Concatenate the new DataFrame with the existing one\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0745969-fadd-4d8d-8356-acd48337d76e",
   "metadata": {},
   "source": [
    "## How to reassemble the previous test set in-case kernel shutdown from here on out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4e818c4-7b45-4c4f-9966-2cd4f3887149",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_google_project_and_credentials('datasets/private/google_translate_apikey.json')\n",
    "json_path = './model_outputs/test/en_to_jp/index.json'\n",
    "engl_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.en',\n",
    "    './datasets/public/pheMT_final/tok.en',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en'\n",
    "]\n",
    "japn_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja',\n",
    "    './datasets/public/pheMT_final/tok.ja',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja'\n",
    "]\n",
    "kftt_phemt_aspec = reassemble_test_dataset(json_path, engl_paths, japn_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4159290-5536-4b72-ad70-ccbfcb8c521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translatedText': 'Later, Jakuhara formed a sect, and so there were two schools of Shomyo in Ohara.\\n', 'detectedSourceLanguage': 'ja', 'input': 'また 、 後 に 寂原 が 一派 を な し て 、 大原 に は 2 派 の 系統 の 声明 が あ っ た 。\\n'}\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'datasets/private/gleaming-glass-421121-19561f1764f8.json'\n",
    "google_translate_and_save(kftt_phemt_aspec, config['project'], config['api_key'], 'Japanese', '') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
