{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0b669-c8a8-4d2c-9eca-96ffa289294d",
   "metadata": {},
   "source": [
    "# Prepare Test set to be benchmarked across different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2efc4e43-56f1-4dc7-a92d-4fa9c3a82ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepl\n",
      "  Downloading deepl-1.17.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from deepl) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2023.11.17)\n",
      "Downloading deepl-1.17.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: deepl\n",
      "Successfully installed deepl-1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae5746-7f4c-48f9-9ba5-9104793eec2e",
   "metadata": {},
   "source": [
    "## Preprocess ASPEC to same format as other benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0b4367-7d83-451c-9af5-5e3735ef0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_aspec_data(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name of the input file (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Create the output file paths in the same directory as the input file\n",
    "    engl_output_path = os.path.join(input_dir, f\"{base_name}.raw.en\")\n",
    "    japn_output_path = os.path.join(input_dir, f\"{base_name}.raw.ja\")\n",
    "\n",
    "    # Open the input file for reading\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Open the output files for writing\n",
    "        with open(engl_output_path, 'w', encoding='utf-8') as engl_file, open(japn_output_path, 'w', encoding='utf-8') as japn_file:\n",
    "            # Iterate over each line in the input file\n",
    "            for line in file:\n",
    "                # Split the line into sections using the delimiter \" ||| \"\n",
    "                sections = line.strip().split(' ||| ')\n",
    "\n",
    "                # Extract the Japanese and English sections (last two sections)\n",
    "                japanese_text = sections[-2]\n",
    "                english_text = sections[-1]\n",
    "\n",
    "                # Write the Japanese text to the Japanese output file\n",
    "                japn_file.write(japanese_text + '\\n')\n",
    "\n",
    "                # Write the English text to the English output file\n",
    "                engl_file.write(english_text + '\\n')\n",
    "\n",
    "    print(f\"Data split completed. Output files: {engl_output_path} and {japn_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73d5e32-48aa-42d5-9b22-ffacd573f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en and ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja\n",
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.en and ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.ja\n"
     ]
    }
   ],
   "source": [
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.txt')\n",
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/test/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53975d6-75d7-40ff-92f9-4b30376aee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name and extension of the input file\n",
    "    base_name, ext = os.path.splitext(os.path.basename(file_path))\n",
    "\n",
    "    # Create the output file path in the same directory as the input file\n",
    "    output_path = os.path.join(input_dir, f\"{base_name}.tok{ext}\")\n",
    "\n",
    "    # Run KyTea on the input file and generate the tokenized output\n",
    "    subprocess.run(['kytea', '-notags', '-wsconst', 'D', '&lt', '-out', 'tok', '<', file_path, '>', output_path], check=True)\n",
    "\n",
    "    print(f\"Tokenization completed. Output file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac01dbdc-aeba-4b5b-93d4-e5a90786bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this function was really slow --> seconds become minutes, just did it in terminal instead kept it fast \n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en')\n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a2ab0-fa39-40bd-b944-cebda16c03a3",
   "metadata": {},
   "source": [
    "## Create massive dataframe with testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b518ecfb-ec7b-454a-850e-cb93a93e9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "random.seed(0) \n",
    "\n",
    "def create_parallel_dataframe(engl_path, japn_path, existing_df=None):\n",
    "    # Read the English and Japanese files\n",
    "    with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "        engl_lines = engl_file.readlines()\n",
    "        japn_lines = japn_file.readlines()\n",
    "\n",
    "    # Get the total number of lines in each file\n",
    "    total_lines = min(len(engl_lines), len(japn_lines))\n",
    "\n",
    "    # Check if there are at least 300 lines in both files\n",
    "    if total_lines < 300:\n",
    "        raise ValueError(\"Files must contain at least 300 lines.\")\n",
    "\n",
    "    # Generate a random starting index for the interval\n",
    "    start_index = random.randint(0, total_lines - 300)\n",
    "\n",
    "    # Extract the random interval of 300 lines from each file\n",
    "    engl_subset = engl_lines[start_index:start_index + 300]\n",
    "    japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "    # Create a new DataFrame with the extracted lines\n",
    "    new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # If an existing DataFrame is provided, concatenate the new data with it\n",
    "    if existing_df is not None:\n",
    "        df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        df = new_df\n",
    "\n",
    "    return df, start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf63cd-08de-43a5-9653-6795dc4b6e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56755041-cf4e-4335-b576-e9139b96d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               English  \\\n",
      "0    Jakugen developed a school later , so there we...   \n",
      "1      Subsequently , Shukai restored Ohara Shomyo .\\n   \n",
      "2    Tanchi established a stream based on a new for...   \n",
      "3    Since then , it became the center of Tendai Sh...   \n",
      "4    The Yuzunembutsu-shu sect , Jodo-shu sect and ...   \n",
      "..                                                 ...   \n",
      "895  Lignin   cresol   extracted   by   acetone   i...   \n",
      "896  A  fiber  mold   u sing   recycled   paper   i...   \n",
      "897  As  a   result ,  a  composite  with   high   ...   \n",
      "898  It   can   be separated into  compound  compos...   \n",
      "899  This  material   reports  the   result   of   ...   \n",
      "\n",
      "                                              Japanese  \n",
      "0    また 、 後 に 寂原 が 一派 を な し て 、 大原 に は 2 派 の 系統 の 声...  \n",
      "1                  のち に 宗快 が 大原 声明 を 再興 する に 至 っ た 。\\n  \n",
      "2             湛智 が 新し い 音楽 理論 に 基づ い た 流れ を 構築 し た 。\\n  \n",
      "3    以降 、 天台 声明 の 中枢 を なし 、 現在 の 天台 声明 に 継承 さ れ て い...  \n",
      "4    融通 念仏 宗 、 浄土 宗 、 浄土 真宗 の 声明 は 、 天台 声明 の 系統 で あ...  \n",
      "..                                                 ...  \n",
      "895  黒松 ， カエデ 材 の 脱脂 木紛 に ， ｐ クレゾール ， 硫酸 を 加え ， アセト...  \n",
      "896  再生 紙 使用 の ファイバ モールド を リグニン クレゾール の 溶液 で 浸漬 ， 溶...  \n",
      "897  その 結果 ， 複合 体 は 高 い 強度 特性 ， 優れ た 疎水 性 及び 寸法 安定 ...  \n",
      "898    なお 単純 な 溶媒 処理 で ， 複合 構成 素材 に 分離 ， 回収 可能 で あ る\\n  \n",
      "899  この 資料 は 平成 ５ 年 の 北海道 ， 東北 の 米穀類 の 冷害 に よ る 農業 ...  \n",
      "\n",
      "[900 rows x 2 columns] 223 516 285\n"
     ]
    }
   ],
   "source": [
    "kftt_out, kftt_start_index = create_parallel_dataframe('./datasets/public/kftt-data-1.0/data/tok/kyoto-test.en', './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja') \n",
    "kftt_and_phemt, phemt_start_index =  create_parallel_dataframe('./datasets/public/pheMT_final/tok.en', './datasets/public/pheMT_final/tok.ja', kftt_out)\n",
    "kftt_phemt_aspec, aspec_start_index = create_parallel_dataframe('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en', './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja', kftt_and_phemt)\n",
    "print(kftt_phemt_aspec, kftt_start_index, phemt_start_index, aspec_start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0fcd1-045b-499f-abae-b69a768c1c1b",
   "metadata": {},
   "source": [
    "# Do some checks on max characters so i dont go over free api limits and go bankrupt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3b4161e-7926-4456-ad1f-b7687bac4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_characters(df):\n",
    "    # Count the total number of characters in the Japanese column\n",
    "    total_japanese_chars = df['Japanese'].str.len().sum()\n",
    "\n",
    "    # Count the total number of characters in the English column\n",
    "    total_english_chars = df['English'].str.len().sum()\n",
    "\n",
    "    # Create a dictionary with the counts\n",
    "    char_counts = {\n",
    "        \"japanese_characters\": total_japanese_chars,\n",
    "        \"english_characters\": total_english_chars\n",
    "    }\n",
    "    print(char_counts) \n",
    "    return char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "835480ea-75ac-44a4-8b2a-519ecdff26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'japanese_characters': 56690, 'english_characters': 128381}\n",
      "Total characters going to API calls = 185071\n"
     ]
    }
   ],
   "source": [
    "tot_chars = count_total_characters(kftt_phemt_aspec.copy())\n",
    "total = tot_chars['japanese_characters'] + tot_chars['english_characters']\n",
    "print(f\"Total characters going to API calls = {total}\")\n",
    "assert total < 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5278baa-22ba-4cd5-aeda-2c7783650a9a",
   "metadata": {},
   "source": [
    "# API tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e5de-e70b-45d6-84f1-67b1e9d6e1e8",
   "metadata": {},
   "source": [
    "## deepL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c17095ef-8622-4486-af42-94f5ca7902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "def translate_and_save(df, api_key, src_language, output_file):\n",
    "    # src_langauge = 'English' or 'Japanese' \n",
    "    try:\n",
    "        # Initialize the DeepL translator\n",
    "        translator = deepl.Translator(api_key)\n",
    "\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                source_text = row[src_language]\n",
    "                target_language = 'EN-US' if src_language== 'Japanese' else 'JA'\n",
    "\n",
    "                # Translate the Japanese text to English\n",
    "                result = translator.translate_text(source_text, target_lang=target_language)\n",
    "                translated_text = result.text\n",
    "\n",
    "                # Write the translated text to the output file\n",
    "                file.write(translated_text + '\\n')\n",
    "\n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except deepl.exceptions.AuthorizationException:\n",
    "        print(\"Invalid API key. Please check your API key.\")\n",
    "\n",
    "    except deepl.exceptions.QuotaExceededException:\n",
    "        print(\"Quota exceeded. Please check your DeepL usage limits.\")\n",
    "\n",
    "    except deepl.exceptions.DeepLException as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a8a3c9-df3b-4034-9de9-ff80cb145944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            api_key = file.readline().strip()\n",
    "            return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error reading file: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54122365-caad-449a-a71a-6a1924f80549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_column(df, column, output_file):\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                japanese_text = row[column]\n",
    "\n",
    "                # Write the Japanese text to the output file\n",
    "                file.write(japanese_text + '\\n')\n",
    "\n",
    "        print(f\"{column} saved to: {output_file}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73ccb8c9-2d33-4b2e-80e7-198ecc317de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json_file(path, kftt_start_index, phe_mt_start_index, aspec_start_index):\n",
    "    # Create a dictionary with the provided indexes\n",
    "    data = {\n",
    "        \"kftt_start_index\": kftt_start_index,\n",
    "        \"phe_mt_start_index\": phe_mt_start_index,\n",
    "        \"aspec_start_index\": aspec_start_index\n",
    "    }\n",
    "    \n",
    "    # Write the dictionary to a file in JSON format\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dc3a599-012b-4c68-b339-31ef20e78bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in and out truth values for jp->en \n",
    "# save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/jp_to_en/in.txt')\n",
    "# save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/jp_to_en/out.txt')\n",
    "# write_json_file('model_outputs/test/jp_to_en/index.json', kftt_start_index, phemt_start_index, aspec_start_index)\n",
    "# Save in and out truth values for en->jp \n",
    "# save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/en_to_jp/in.txt')\n",
    "# save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/en_to_jp/out.txt')\n",
    "# write_json_file('model_outputs/test/en_to_jp/index.json', kftt_start_index, phemt_start_index, aspec_start_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a55544f4-bf31-4d2f-8b55-7455332df680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. Translations saved to: model_outputs/test/jp_to_en/deepL/out.txt\n",
      "Original Japanese: また 、 後 に 寂原 が 一派 を な し て 、 大原 に は 2 派 の 系統 の 声明 が あ っ た 。\n",
      "\n",
      "Translated English: None\n"
     ]
    }
   ],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "api_key = get_api_key('./datasets/private/apikey.txt')\n",
    "translated_english = translate_and_save(kftt_phemt_aspec, api_key, 'Japanese', 'model_outputs/test/jp_to_en/deepL/out.txt')\n",
    "print(\"Original Japanese:\", kftt_phemt_aspec['Japanese'].iloc[0])\n",
    "print(\"Translated English:\", translated_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbf31ee8-437f-41df-8d80-48dafd07df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "# api_key = get_api_key('./datasets/private/apikey.txt') \n",
    "# print(api_key) \n",
    "# print(kftt_phemt_aspec) \n",
    "# translated_japanese = translate_and_save(kftt_phemt_aspec, api_key, 'English', 'model_outputs/test/en_to_jp/deepL/out.txt') \n",
    "# print(\"Original English:\", kftt_phemt_aspec['English'].iloc[0])\n",
    "# print(\"Translated Japanese:\", translated_japanese)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b689d22-1781-4274-8fef-bf9fcc918b39",
   "metadata": {},
   "source": [
    "## Evaluation Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "691bfbda-984a-49e5-9d14-0cee81ff1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu==2.0.0\n",
      "  Obtaining dependency information for sacrebleu==2.0.0 from https://files.pythonhosted.org/packages/fa/63/b3c11f951eafa2dc296862431f29fb12dbe191cb72217cf88ed04c32086b/sacrebleu-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached sacrebleu-2.0.0-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: portalocker in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (2.8.2)\n",
      "Requirement already satisfied: regex in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (2024.4.16)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (1.26.4)\n",
      "Requirement already satisfied: colorama in /Users/thomaspett/miniforge3/lib/python3.10/site-packages (from sacrebleu==2.0.0) (0.4.6)\n",
      "Using cached sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "Installing collected packages: sacrebleu\n",
      "Successfully installed sacrebleu-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sacrebleu==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cc5434d-abd0-4cfe-885f-a9e2c2541f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu[ja] in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: portalocker in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (2.8.2)\n",
      "Requirement already satisfied: regex in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (1.26.2)\n",
      "Requirement already satisfied: colorama in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (0.4.6)\n",
      "Requirement already satisfied: lxml in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from sacrebleu[ja]) (5.2.1)\n",
      "Collecting mecab-python3<=1.0.6,>=1.0.5 (from sacrebleu[ja])\n",
      "  Downloading mecab-python3-1.0.6.tar.gz (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ipadic<2.0,>=1.0 (from sacrebleu[ja])\n",
      "  Using cached ipadic-1.0.0-py3-none-any.whl\n",
      "Building wheels for collected packages: mecab-python3\n",
      "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[21 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages/setuptools/__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001b[31m   \u001b[0m WARNING setuptools_scm.pyproject_reading toml section missing 'pyproject.toml does not contain a tool.setuptools_scm section'\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.0-arm64-cpython-310/MeCab\n",
      "  \u001b[31m   \u001b[0m copying src/MeCab/__init__.py -> build/lib.macosx-11.0-arm64-cpython-310/MeCab\n",
      "  \u001b[31m   \u001b[0m copying src/MeCab/cli.py -> build/lib.macosx-11.0-arm64-cpython-310/MeCab\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m error: [Errno 2] No such file or directory: 'mecab-config'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for mecab-python3\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for mecab-python3\n",
      "Failed to build mecab-python3\n",
      "\u001b[31mERROR: Could not build wheels for mecab-python3, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"sacrebleu[ja]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22cd6041-f8b8-49a5-bb65-8454c22e6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "def evaluate_translation(input_file, translated_file, reference_file):\n",
    "    # Function to read a file and extract non-blank lines\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "        return lines\n",
    "\n",
    "    # Read the files\n",
    "    input_lines = read_file(input_file)\n",
    "    translated_lines = read_file(translated_file)\n",
    "    reference_lines = [read_file(reference_file)]  # Note the list wrapping for multiple references support\n",
    "    print(translated_lines[1]) # TODO remove\n",
    "    print(reference_lines[0][1]) # TODO remove\n",
    "\n",
    "    # Initialize the BLEU object\n",
    "    bleu = BLEU(tokenize='ja-mecab')\n",
    "\n",
    "    # Compute the BLEU score\n",
    "    score = bleu.corpus_score(translated_lines, reference_lines)\n",
    "\n",
    "    # Print and return the BLEU score and its detailed breakdown\n",
    "    print(f\"Bleu Score: {score.score}\")\n",
    "    print(f\"Full report: {score}\")\n",
    "    return score.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc059297-4954-4dcd-a9d3-6bd079e543ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Later, Soukai revived the Ohara-voice.\n",
      "Subsequently , Shukai restored Ohara Shomyo .\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\nJapanese tokenization requires extra dependencies, but you do not have them installed.\nPlease install them like so.\n\n    pip install sacrebleu[ja]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mevaluate_translation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_outputs/test/jp_to_en/in.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslated_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_outputs/test/jp_to_en/deepL/out.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_outputs/test/jp_to_en/out.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluate_translation(input_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/en_to_jp/in.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, translated_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/en_to_jp/deepL/out.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, reference_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/en_to_jp/out.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36mevaluate_translation\u001b[0;34m(input_file, translated_file, reference_file)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(reference_lines[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;66;03m# TODO remove\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize the BLEU object\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m bleu \u001b[38;5;241m=\u001b[39m \u001b[43mBLEU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mja-mecab\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compute the BLEU score\u001b[39;00m\n\u001b[1;32m     20\u001b[0m score \u001b[38;5;241m=\u001b[39m bleu\u001b[38;5;241m.\u001b[39mcorpus_score(translated_lines, reference_lines)\n",
      "File \u001b[0;32m~/Desktop/pytorch-test/env/lib/python3.10/site-packages/sacrebleu/metrics/bleu.py:201\u001b[0m, in \u001b[0;36mBLEU.__init__\u001b[0;34m(self, lowercase, force, tokenize, smooth_method, smooth_value, max_ngram_order, effective_order, trg_lang, references)\u001b[0m\n\u001b[1;32m    197\u001b[0m         sacrelogger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mko-mecab\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspm\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m tokenizer for Korean.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Create the tokenizer\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_tokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Build the signature\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msignature()\n",
      "File \u001b[0;32m~/Desktop/pytorch-test/env/lib/python3.10/site-packages/sacrebleu/tokenizers/tokenizer_ja_mecab.py:23\u001b[0m, in \u001b[0;36mTokenizerJaMecab.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MeCab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(FAIL_MESSAGE)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger \u001b[38;5;241m=\u001b[39m MeCab\u001b[38;5;241m.\u001b[39mTagger(ipadic\u001b[38;5;241m.\u001b[39mMECAB_ARGS \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m -Owakati\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# make sure the dictionary is IPA\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nJapanese tokenization requires extra dependencies, but you do not have them installed.\nPlease install them like so.\n\n    pip install sacrebleu[ja]\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_translation(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/deepL/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt'))\n",
    "print(evaluate_translation(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/deepL/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de90a88-60e1-4eee-8299-cca6aa5724cc",
   "metadata": {},
   "source": [
    "## Google Translate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7888cde-2c4e-4050-a9fd-edd4dfa14f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
