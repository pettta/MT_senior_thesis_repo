{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0b669-c8a8-4d2c-9eca-96ffa289294d",
   "metadata": {},
   "source": [
    "# Prepare Test set to be benchmarked across different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2efc4e43-56f1-4dc7-a92d-4fa9c3a82ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepl\n",
      "  Downloading deepl-1.17.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from deepl) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2023.11.17)\n",
      "Downloading deepl-1.17.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: deepl\n",
      "Successfully installed deepl-1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae5746-7f4c-48f9-9ba5-9104793eec2e",
   "metadata": {},
   "source": [
    "## Preprocess ASPEC to same format as other benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0b4367-7d83-451c-9af5-5e3735ef0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_aspec_data(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name of the input file (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Create the output file paths in the same directory as the input file\n",
    "    engl_output_path = os.path.join(input_dir, f\"{base_name}.raw.en\")\n",
    "    japn_output_path = os.path.join(input_dir, f\"{base_name}.raw.ja\")\n",
    "\n",
    "    # Open the input file for reading\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Open the output files for writing\n",
    "        with open(engl_output_path, 'w', encoding='utf-8') as engl_file, open(japn_output_path, 'w', encoding='utf-8') as japn_file:\n",
    "            # Iterate over each line in the input file\n",
    "            for line in file:\n",
    "                # Split the line into sections using the delimiter \" ||| \"\n",
    "                sections = line.strip().split(' ||| ')\n",
    "\n",
    "                # Extract the Japanese and English sections (last two sections)\n",
    "                japanese_text = sections[-2]\n",
    "                english_text = sections[-1]\n",
    "\n",
    "                # Write the Japanese text to the Japanese output file\n",
    "                japn_file.write(japanese_text + '\\n')\n",
    "\n",
    "                # Write the English text to the English output file\n",
    "                engl_file.write(english_text + '\\n')\n",
    "\n",
    "    print(f\"Data split completed. Output files: {engl_output_path} and {japn_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73d5e32-48aa-42d5-9b22-ffacd573f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en and ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja\n",
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.en and ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.ja\n"
     ]
    }
   ],
   "source": [
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.txt')\n",
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/test/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53975d6-75d7-40ff-92f9-4b30376aee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name and extension of the input file\n",
    "    base_name, ext = os.path.splitext(os.path.basename(file_path))\n",
    "\n",
    "    # Create the output file path in the same directory as the input file\n",
    "    output_path = os.path.join(input_dir, f\"{base_name}.tok{ext}\")\n",
    "\n",
    "    # Run KyTea on the input file and generate the tokenized output\n",
    "    subprocess.run(['kytea', '-notags', '-wsconst', 'D', '&lt', '-out', 'tok', '<', file_path, '>', output_path], check=True)\n",
    "\n",
    "    print(f\"Tokenization completed. Output file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac01dbdc-aeba-4b5b-93d4-e5a90786bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this function was really slow --> seconds become minutes, just did it in terminal instead kept it fast \n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en')\n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a2ab0-fa39-40bd-b944-cebda16c03a3",
   "metadata": {},
   "source": [
    "## Create massive dataframe with testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b518ecfb-ec7b-454a-850e-cb93a93e9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "random.seed(0) \n",
    "\n",
    "def create_parallel_dataframe(engl_path, japn_path, existing_df=None):\n",
    "    # Read the English and Japanese files\n",
    "    with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "        engl_lines = engl_file.readlines()\n",
    "        japn_lines = japn_file.readlines()\n",
    "\n",
    "    # Get the total number of lines in each file\n",
    "    total_lines = min(len(engl_lines), len(japn_lines))\n",
    "\n",
    "    # Check if there are at least 300 lines in both files\n",
    "    if total_lines < 300:\n",
    "        raise ValueError(\"Files must contain at least 300 lines.\")\n",
    "\n",
    "    # Generate a random starting index for the interval\n",
    "    start_index = random.randint(0, total_lines - 300)\n",
    "\n",
    "    # Extract the random interval of 300 lines from each file\n",
    "    engl_subset = engl_lines[start_index:start_index + 300]\n",
    "    japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "    # Create a new DataFrame with the extracted lines\n",
    "    new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # If an existing DataFrame is provided, concatenate the new data with it\n",
    "    if existing_df is not None:\n",
    "        df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        df = new_df\n",
    "\n",
    "    return df, start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf63cd-08de-43a5-9653-6795dc4b6e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56755041-cf4e-4335-b576-e9139b96d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               English  \\\n",
      "0    Jakugen developed a school later , so there we...   \n",
      "1      Subsequently , Shukai restored Ohara Shomyo .\\n   \n",
      "2    Tanchi established a stream based on a new for...   \n",
      "3    Since then , it became the center of Tendai Sh...   \n",
      "4    The Yuzunembutsu-shu sect , Jodo-shu sect and ...   \n",
      "..                                                 ...   \n",
      "895  Lignin   cresol   extracted   by   acetone   i...   \n",
      "896  A  fiber  mold   u sing   recycled   paper   i...   \n",
      "897  As  a   result ,  a  composite  with   high   ...   \n",
      "898  It   can   be separated into  compound  compos...   \n",
      "899  This  material   reports  the   result   of   ...   \n",
      "\n",
      "                                              Japanese  \n",
      "0    また 、 後 に 寂原 が 一派 を な し て 、 大原 に は 2 派 の 系統 の 声...  \n",
      "1                  のち に 宗快 が 大原 声明 を 再興 する に 至 っ た 。\\n  \n",
      "2             湛智 が 新し い 音楽 理論 に 基づ い た 流れ を 構築 し た 。\\n  \n",
      "3    以降 、 天台 声明 の 中枢 を なし 、 現在 の 天台 声明 に 継承 さ れ て い...  \n",
      "4    融通 念仏 宗 、 浄土 宗 、 浄土 真宗 の 声明 は 、 天台 声明 の 系統 で あ...  \n",
      "..                                                 ...  \n",
      "895  黒松 ， カエデ 材 の 脱脂 木紛 に ， ｐ クレゾール ， 硫酸 を 加え ， アセト...  \n",
      "896  再生 紙 使用 の ファイバ モールド を リグニン クレゾール の 溶液 で 浸漬 ， 溶...  \n",
      "897  その 結果 ， 複合 体 は 高 い 強度 特性 ， 優れ た 疎水 性 及び 寸法 安定 ...  \n",
      "898    なお 単純 な 溶媒 処理 で ， 複合 構成 素材 に 分離 ， 回収 可能 で あ る\\n  \n",
      "899  この 資料 は 平成 ５ 年 の 北海道 ， 東北 の 米穀類 の 冷害 に よ る 農業 ...  \n",
      "\n",
      "[900 rows x 2 columns] 223 516 285\n"
     ]
    }
   ],
   "source": [
    "kftt_out, kftt_start_index = create_parallel_dataframe('./datasets/public/kftt-data-1.0/data/tok/kyoto-test.en', './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja') \n",
    "kftt_and_phemt, phemt_start_index =  create_parallel_dataframe('./datasets/public/pheMT_final/tok.en', './datasets/public/pheMT_final/tok.ja', kftt_out)\n",
    "kftt_phemt_aspec, aspec_start_index = create_parallel_dataframe('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en', './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja', kftt_and_phemt)\n",
    "print(kftt_phemt_aspec, kftt_start_index, phemt_start_index, aspec_start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0fcd1-045b-499f-abae-b69a768c1c1b",
   "metadata": {},
   "source": [
    "# Do some checks on max characters so i dont go over free api limits and go bankrupt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3b4161e-7926-4456-ad1f-b7687bac4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_characters(df):\n",
    "    # Count the total number of characters in the Japanese column\n",
    "    total_japanese_chars = df['Japanese'].str.len().sum()\n",
    "\n",
    "    # Count the total number of characters in the English column\n",
    "    total_english_chars = df['English'].str.len().sum()\n",
    "\n",
    "    # Create a dictionary with the counts\n",
    "    char_counts = {\n",
    "        \"japanese_characters\": total_japanese_chars,\n",
    "        \"english_characters\": total_english_chars\n",
    "    }\n",
    "    print(char_counts) \n",
    "    return char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "835480ea-75ac-44a4-8b2a-519ecdff26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'japanese_characters': 56690, 'english_characters': 128381}\n",
      "Total characters going to API calls = 185071\n"
     ]
    }
   ],
   "source": [
    "tot_chars = count_total_characters(kftt_phemt_aspec.copy())\n",
    "total = tot_chars['japanese_characters'] + tot_chars['english_characters']\n",
    "print(f\"Total characters going to API calls = {total}\")\n",
    "assert total < 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5278baa-22ba-4cd5-aeda-2c7783650a9a",
   "metadata": {},
   "source": [
    "# API tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e5de-e70b-45d6-84f1-67b1e9d6e1e8",
   "metadata": {},
   "source": [
    "## deepL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c17095ef-8622-4486-af42-94f5ca7902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "def translate_and_save(df, api_key, src_language, output_file):\n",
    "    # src_langauge = 'English' or 'Japanese' \n",
    "    try:\n",
    "        # Initialize the DeepL translator\n",
    "        translator = deepl.Translator(api_key)\n",
    "\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                source_text = row[src_language]\n",
    "                target_language = 'EN-US' if src_language== 'Japanese' else 'JA'\n",
    "\n",
    "                # Translate the Japanese text to English\n",
    "                result = translator.translate_text(source_text, target_lang=target_language)\n",
    "                translated_text = result.text\n",
    "\n",
    "                # Write the translated text to the output file\n",
    "                file.write(translated_text + '\\n')\n",
    "\n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except deepl.exceptions.AuthorizationException:\n",
    "        print(\"Invalid API key. Please check your API key.\")\n",
    "\n",
    "    except deepl.exceptions.QuotaExceededException:\n",
    "        print(\"Quota exceeded. Please check your DeepL usage limits.\")\n",
    "\n",
    "    except deepl.exceptions.DeepLException as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a8a3c9-df3b-4034-9de9-ff80cb145944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            api_key = file.readline().strip()\n",
    "            return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error reading file: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54122365-caad-449a-a71a-6a1924f80549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_column(df, column, output_file):\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                japanese_text = row[column]\n",
    "\n",
    "                # Write the Japanese text to the output file\n",
    "                file.write(japanese_text + '\\n')\n",
    "\n",
    "        print(f\"{column} saved to: {output_file}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73ccb8c9-2d33-4b2e-80e7-198ecc317de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json_file(path, kftt_start_index, phe_mt_start_index, aspec_start_index):\n",
    "    # Create a dictionary with the provided indexes\n",
    "    data = {\n",
    "        \"kftt_start_index\": kftt_start_index,\n",
    "        \"phe_mt_start_index\": phe_mt_start_index,\n",
    "        \"aspec_start_index\": aspec_start_index\n",
    "    }\n",
    "    \n",
    "    # Write the dictionary to a file in JSON format\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dc3a599-012b-4c68-b339-31ef20e78bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in and out truth values for jp->en \n",
    "# save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/jp_to_en/in.txt')\n",
    "# save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/jp_to_en/out.txt')\n",
    "# write_json_file('model_outputs/test/jp_to_en/index.json', kftt_start_index, phemt_start_index, aspec_start_index)\n",
    "# Save in and out truth values for en->jp \n",
    "# save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/en_to_jp/in.txt')\n",
    "# save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/en_to_jp/out.txt')\n",
    "# write_json_file('model_outputs/test/en_to_jp/index.json', kftt_start_index, phemt_start_index, aspec_start_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a55544f4-bf31-4d2f-8b55-7455332df680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. Translations saved to: model_outputs/test/jp_to_en/deepL/out.txt\n",
      "Original Japanese: また 、 後 に 寂原 が 一派 を な し て 、 大原 に は 2 派 の 系統 の 声明 が あ っ た 。\n",
      "\n",
      "Translated English: None\n"
     ]
    }
   ],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "api_key = get_api_key('./datasets/private/apikey.txt')\n",
    "translated_english = translate_and_save(kftt_phemt_aspec, api_key, 'Japanese', 'model_outputs/test/jp_to_en/deepL/out.txt')\n",
    "print(\"Original Japanese:\", kftt_phemt_aspec['Japanese'].iloc[0])\n",
    "print(\"Translated English:\", translated_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbf31ee8-437f-41df-8d80-48dafd07df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "# api_key = get_api_key('./datasets/private/apikey.txt') \n",
    "# print(api_key) \n",
    "# print(kftt_phemt_aspec) \n",
    "# translated_japanese = translate_and_save(kftt_phemt_aspec, api_key, 'English', 'model_outputs/test/en_to_jp/deepL/out.txt') \n",
    "# print(\"Original English:\", kftt_phemt_aspec['English'].iloc[0])\n",
    "# print(\"Translated Japanese:\", translated_japanese)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b689d22-1781-4274-8fef-bf9fcc918b39",
   "metadata": {},
   "source": [
    "## Evaluation Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "691bfbda-984a-49e5-9d14-0cee81ff1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Obtaining dependency information for sacrebleu from https://files.pythonhosted.org/packages/df/d5/f07d3c37bd98db883330276d77e7b04b6c50564c68fb95a76e05422a2850/sacrebleu-2.4.2-py3-none-any.whl.metadata\n",
      "  Using cached sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting regex (from sacrebleu)\n",
      "  Obtaining dependency information for regex from https://files.pythonhosted.org/packages/f1/4b/0477c6076fa63a8f3261e89c69765d5369fb70be644b6df844569970c1a7/regex-2024.4.16-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached regex-2024.4.16-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Obtaining dependency information for tabulate>=0.8.9 from https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl.metadata\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting numpy>=1.17 (from sacrebleu)\n",
      "  Obtaining dependency information for numpy>=1.17 from https://files.pythonhosted.org/packages/20/f7/b24208eba89f9d1b58c1668bc6c8c4fd472b20c45573cb767f59d49fb0f6/numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata\n",
      "  Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Obtaining dependency information for colorama from https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl.metadata\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Obtaining dependency information for lxml from https://files.pythonhosted.org/packages/65/04/b895658bd5c26203802ac197ce674fa9d1ce494559cf06db4fd19230526a/lxml-5.2.1-cp310-cp310-macosx_10_9_universal2.whl.metadata\n",
      "  Using cached lxml-5.2.1-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Using cached sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached lxml-5.2.1-cp310-cp310-macosx_10_9_universal2.whl (8.5 MB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached regex-2024.4.16-cp310-cp310-macosx_11_0_arm64.whl (291 kB)\n",
      "Installing collected packages: tabulate, regex, portalocker, numpy, lxml, colorama, sacrebleu\n",
      "  Attempting uninstall: tabulate\n",
      "    Found existing installation: tabulate 0.9.0\n",
      "    Uninstalling tabulate-0.9.0:\n",
      "      Successfully uninstalled tabulate-0.9.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.4.16\n",
      "    Uninstalling regex-2024.4.16:\n",
      "      Successfully uninstalled regex-2024.4.16\n",
      "  Attempting uninstall: portalocker\n",
      "    Found existing installation: portalocker 2.8.2\n",
      "    Uninstalling portalocker-2.8.2:\n",
      "      Successfully uninstalled portalocker-2.8.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.2.1\n",
      "    Uninstalling lxml-5.2.1:\n",
      "      Successfully uninstalled lxml-5.2.1\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "  Attempting uninstall: sacrebleu\n",
      "    Found existing installation: sacrebleu 2.4.2\n",
      "    Uninstalling sacrebleu-2.4.2:\n",
      "      Successfully uninstalled sacrebleu-2.4.2\n",
      "Successfully installed colorama-0.4.6 lxml-5.2.1 numpy-1.26.4 portalocker-2.8.2 regex-2024.4.16 sacrebleu-2.4.2 tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22cd6041-f8b8-49a5-bb65-8454c22e6de2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'portalocker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msacrebleu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msacrebleu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BLEU\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_translation\u001b[39m(input_file, translated_file, reference_file):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Function to read a file and extract non-blank lines\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_file\u001b[39m(file_path):\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/sacrebleu/sacrebleu/__init__.py:21\u001b[0m\n\u001b[1;32m     17\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.4.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m __description__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHassle-free computation of shareable, comparable, and reproducible BLEU, chrF, and TER scores\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m smart_open, SACREBLEU_DIR, download_test_set\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_source_file, get_reference_files\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_available_testsets, get_langpairs_for_testset\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/sacrebleu/sacrebleu/utils.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhashlib\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mportalocker\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Sequence, Dict\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'portalocker'"
     ]
    }
   ],
   "source": [
    "from sacrebleu.sacrebleu.metrics import BLEU\n",
    "\n",
    "def evaluate_translation(input_file, translated_file, reference_file):\n",
    "    # Function to read a file and extract non-blank lines\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "        return lines\n",
    "\n",
    "    # Read the files\n",
    "    input_lines = read_file(input_file)\n",
    "    translated_lines = read_file(translated_file)\n",
    "    reference_lines = [read_file(reference_file)]  # Note the list wrapping for multiple references support\n",
    "\n",
    "    # Initialize the BLEU object\n",
    "    bleu = BLEU()\n",
    "\n",
    "    # Compute the BLEU score\n",
    "    score = bleu.corpus_score(translated_lines, reference_lines)\n",
    "\n",
    "    # Print and return the BLEU score and its detailed breakdown\n",
    "    print(f\"Bleu Score: {score.score}\")\n",
    "    print(f\"Full report: {score}\")\n",
    "    return score.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc059297-4954-4dcd-a9d3-6bd079e543ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_translation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mevaluate_translation\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/jp_to_en/in.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/jp_to_en/deepL/out.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/jp_to_en/out.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_translation' is not defined"
     ]
    }
   ],
   "source": [
    "print(evaluate_translation('model_outputs/test/jp_to_en/in.txt', 'model_outputs/test/jp_to_en/deepL/out.txt', 'model_outputs/test/jp_to_en/out.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de90a88-60e1-4eee-8299-cca6aa5724cc",
   "metadata": {},
   "source": [
    "## Google Translate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7888cde-2c4e-4050-a9fd-edd4dfa14f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
