{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c59591c-3929-4ed9-b73f-68b9471fe2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.1\n",
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea37182-c812-4ec8-ad49-035d98380dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6307, 0.9091, 0.2729, 0.1627],\n",
       "        [0.4516, 0.5932, 0.7232, 0.3652],\n",
       "        [0.6988, 0.2445, 0.5683, 0.3595]], device='mps:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data and send it to the device\n",
    "x = torch.rand(size=(3, 4)).to(device)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53fd432c-d8f5-46f4-93bf-68aa9a6a4cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlx-examples'...\n",
      "remote: Enumerating objects: 1504, done.\u001b[K\n",
      "remote: Counting objects: 100% (644/644), done.\u001b[K\n",
      "remote: Compressing objects: 100% (238/238), done.\u001b[K\n",
      "remote: Total 1504 (delta 494), reused 456 (delta 402), pack-reused 860\u001b[K\n",
      "Receiving objects: 100% (1504/1504), 1.99 MiB | 3.80 MiB/s, done.\n",
      "Resolving deltas: 100% (934/934), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ml-explore/mlx-examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a09be3-e5b6-41f2-b813-0405dfda40bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlx>=0.0.7 (from -r mlx-examples/lora/requirements.txt (line 1))\n",
      "  Downloading mlx-0.0.10-cp310-cp310-macosx_14_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting transformers (from -r mlx-examples/lora/requirements.txt (line 2))\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./env/lib/python3.10/site-packages (from -r mlx-examples/lora/requirements.txt (line 3)) (1.26.2)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.10/site-packages (from transformers->-r mlx-examples/lora/requirements.txt (line 2)) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers->-r mlx-examples/lora/requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.10/site-packages (from transformers->-r mlx-examples/lora/requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.10/site-packages (from transformers->-r mlx-examples/lora/requirements.txt (line 2)) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r mlx-examples/lora/requirements.txt (line 2))\n",
      "  Downloading regex-2023.12.25-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in ./env/lib/python3.10/site-packages (from transformers->-r mlx-examples/lora/requirements.txt (line 2)) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers->-r mlx-examples/lora/requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers->-r mlx-examples/lora/requirements.txt (line 2))\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.10/site-packages (from transformers->-r mlx-examples/lora/requirements.txt (line 2)) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r mlx-examples/lora/requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r mlx-examples/lora/requirements.txt (line 2)) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests->transformers->-r mlx-examples/lora/requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.10/site-packages (from requests->transformers->-r mlx-examples/lora/requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests->transformers->-r mlx-examples/lora/requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.10/site-packages (from requests->transformers->-r mlx-examples/lora/requirements.txt (line 2)) (2023.11.17)\n",
      "Downloading mlx-0.0.10-cp310-cp310-macosx_14_0_arm64.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, mlx, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.20.2 mlx-0.0.10 regex-2023.12.25 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r mlx-examples/lora/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571392f1-43fd-4c19-bd3e-7b37f40eef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'seamless_communication'...\n",
      "remote: Enumerating objects: 4658, done.\u001b[K\n",
      "remote: Counting objects: 100% (864/864), done.\u001b[K\n",
      "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
      "remote: Total 4658 (delta 761), reused 659 (delta 654), pack-reused 3794\u001b[K\n",
      "Receiving objects: 100% (4658/4658), 12.05 MiB | 16.43 MiB/s, done.\n",
      "Resolving deltas: 100% (2957/2957), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/seamless_communication.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf592df5-e508-4b2f-9411-50319fdd5ea5",
   "metadata": {},
   "source": [
    "# DONE\n",
    "- Add to slides these \n",
    "- MTNT (2018) https://github.com/pmichel31415/mtnt (Japanese to English and French) &#x2611;\n",
    "- JESC (2017) https://arxiv.org/abs/1710.10639v4 &#x2611;\n",
    "- PHE-MT (2020): https://github.com/cl-tohoku/PheMT?tab=readme-ov-file based on MTNT &#x2611;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b4e82-3e26-4b94-8af4-3b51f8ec1e9a",
   "metadata": {},
   "source": [
    "# TODO Experiments\n",
    "- First look at differences between spBLEU settings in pheMT paper and SeamlessMT paper\n",
    "- If they are different, then standardize these \n",
    "- Evaluate NLBB MoE, M4TLarge v1 and v2 on the 4 datasets from the PheMT paper, and compare with the BLEU results from there\n",
    "- Evaluate Google translate and deepL for those datasets using the chrF2++ metric \n",
    "- Assemble some sort of dataset combining PheMT and JParaCrawl v3.0\n",
    "- Look into which is the best way to finetune this (lora, qlora, instruction tuning), and max viable model size I can work on\n",
    "- Also look into if they provide some sort of fine tuning support built in, so I dont waste additional time\n",
    "- Fine tune that model in that way using that dataset, and compare if results are better than others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3908ea9-2890-41ac-bca9-680f78e7a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PheMT'...\n",
      "remote: Enumerating objects: 67, done.\u001b[K\n",
      "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
      "remote: Total 67 (delta 31), reused 51 (delta 18), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (67/67), 2.14 MiB | 14.32 MiB/s, done.\n",
      "Resolving deltas: 100% (31/31), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cl-tohoku/PheMT.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
