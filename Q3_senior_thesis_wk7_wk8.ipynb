{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9834ac2f-7a0c-415c-8951-3074cc55a08a",
   "metadata": {},
   "source": [
    "# Organize all of the benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45241c17-7a4f-4eec-ba00-15d86034c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3b9ff8a-99c7-415c-8f6e-18c5c3d0d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sacrebleu.metrics import BLEU, CHRF \n",
    "# def evaluate_translation_bleu(input_file, translated_file, reference_file):\n",
    "#     # Function to read a file and extract non-blank lines\n",
    "#     def read_file(file_path):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             lines = [line.strip() for line in file if line.strip()]\n",
    "#         return lines\n",
    "\n",
    "#     # Read the files\n",
    "#     input_lines = read_file(input_file)\n",
    "#     translated_lines = read_file(translated_file)\n",
    "#     reference_lines = [read_file(reference_file)]  # Note the list wrapping for multiple references support\n",
    "#     #print(translated_lines[1]) # TODO remove\n",
    "#     #print(reference_lines[0][1]) # TODO remove\n",
    "\n",
    "#     # Initialize the BLEU object\n",
    "#     bleu = BLEU()\n",
    "#     chrf = CHRF() \n",
    "\n",
    "#     # Compute the BLEU score\n",
    "#     score = bleu.corpus_score(translated_lines, reference_lines)\n",
    "#     score2 = chrf.corpus_score(translated_lines, reference_lines)\n",
    "\n",
    "#     # Print and return the BLEU score and its detailed breakdown\n",
    "#     #print(f\"Bleu Score: {score.score}\")\n",
    "#     #print(f\"CHRF Score: {score2.score}\")\n",
    "#     #print(f\"Full report 1: {score}\")\n",
    "#     #print(f\"Full report 2: {score2}\")\n",
    "#     return (score.score, score2.score) \n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "def evaluate_translation_bleu(input_file, translated_file, reference_file):\n",
    "    # Function to read a file and extract non-blank lines\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "        return lines\n",
    "\n",
    "    # Function to split a list into thirds\n",
    "    def split_into_thirds(lst):\n",
    "        n = len(lst)\n",
    "        third = n // 3\n",
    "        return lst[:third], lst[third:2*third], lst[2*third:]\n",
    "\n",
    "    # Read the files\n",
    "    input_lines = read_file(input_file)\n",
    "    translated_lines = read_file(translated_file)\n",
    "    reference_lines = [read_file(reference_file)]  # Note the list wrapping for multiple references support\n",
    "\n",
    "    # Split the data into thirds\n",
    "    input_thirds = split_into_thirds(input_lines)\n",
    "    translated_thirds = split_into_thirds(translated_lines)\n",
    "    reference_thirds = [split_into_thirds(ref) for ref in reference_lines]\n",
    "\n",
    "    # Initialize the BLEU and CHRF objects\n",
    "    bleu = BLEU()\n",
    "    chrf = CHRF()\n",
    "\n",
    "    # Function to compute scores for a given set of lines\n",
    "    def compute_scores(translated, reference):\n",
    "        bleu_score = bleu.corpus_score(translated, reference).score\n",
    "        chrf_score = chrf.corpus_score(translated, reference).score\n",
    "        return bleu_score, chrf_score\n",
    "\n",
    "    # Compute scores for each third and the whole dataset\n",
    "    scores = {\n",
    "        \"whole_dataset\": compute_scores(translated_lines, reference_lines),\n",
    "        \"first_third\": compute_scores(translated_thirds[0], [ref[0] for ref in reference_thirds]),\n",
    "        \"second_third\": compute_scores(translated_thirds[1], [ref[1] for ref in reference_thirds]),\n",
    "        \"third_third\": compute_scores(translated_thirds[2], [ref[2] for ref in reference_thirds])\n",
    "    }\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Example usage\n",
    "# scores = evaluate_translation_bleu('input.txt', 'translated.txt', 'reference.txt')\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42b769cf-8f05-45bb-920e-e58716b5f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet import download_model, load_from_checkpoint\n",
    "# import torch\n",
    "\n",
    "# def evaluate_translation_comet(input_file, translated_file, reference_file):\n",
    "#     # Function to read a file and extract non-blank lines\n",
    "#     def read_file(file_path):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             lines = [line.strip() for line in file if line.strip()]\n",
    "#         return lines\n",
    "\n",
    "#     # Read the files\n",
    "#     input_lines = read_file(input_file)\n",
    "#     translated_lines = read_file(translated_file)\n",
    "#     reference_lines = read_file(reference_file)  # No need to wrap in a list for COMET\n",
    "\n",
    "#     # Initialize the COMET model\n",
    "#     model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "#     comet_model = load_from_checkpoint(model_path)\n",
    "\n",
    "#     # Prepare data for COMET\n",
    "#     data = [{\"src\": src, \"mt\": mt, \"ref\": ref} for src, mt, ref in zip(input_lines, translated_lines, reference_lines)]\n",
    "\n",
    "#     # Compute COMET scores\n",
    "#     #print(\"DATA BEING TESTED:\", data) \n",
    "#     model_output = comet_model.predict(data, gpus=0)\n",
    "\n",
    "#     # Print COMET scores\n",
    "#     print(f\"COMET output:\", model_output) \n",
    "#     return model_output\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import torch\n",
    "\n",
    "def evaluate_translation_comet(input_file, translated_file, reference_file):\n",
    "    # Function to read a file and extract non-blank lines\n",
    "    def read_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = [line.strip() for line in file if line.strip()]\n",
    "        return lines\n",
    "\n",
    "    # Function to split a list into thirds\n",
    "    def split_into_thirds(lst):\n",
    "        n = len(lst)\n",
    "        third = n // 3\n",
    "        return lst[:third], lst[third:2*third], lst[2*third:]\n",
    "\n",
    "    # Function to calculate the average of a list\n",
    "    def calculate_average(lst):\n",
    "        return sum(lst) / len(lst) if lst else 0\n",
    "\n",
    "    # Read the files\n",
    "    input_lines = read_file(input_file)\n",
    "    translated_lines = read_file(translated_file)\n",
    "    reference_lines = read_file(reference_file)  # No need to wrap in a list for COMET\n",
    "\n",
    "    # Initialize the COMET model\n",
    "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    comet_model = load_from_checkpoint(model_path)\n",
    "\n",
    "    # Prepare data for COMET\n",
    "    data = [{\"src\": src, \"mt\": mt, \"ref\": ref} for src, mt, ref in zip(input_lines, translated_lines, reference_lines)]\n",
    "\n",
    "    # Compute COMET scores\n",
    "    model_output = comet_model.predict(data, gpus=0)\n",
    "    scores = model_output['scores']\n",
    "\n",
    "    # Split scores into thirds\n",
    "    first_third, second_third, third_third = split_into_thirds(scores)\n",
    "\n",
    "    # Calculate average scores for each third and the whole dataset\n",
    "    average_scores = {\n",
    "        \"whole_dataset\": calculate_average(scores),\n",
    "        \"first_third\": calculate_average(first_third),\n",
    "        \"second_third\": calculate_average(second_third),\n",
    "        \"third_third\": calculate_average(third_third)\n",
    "    }\n",
    "\n",
    "    return average_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917fe2e-0f99-447b-abac-6fc12ad62b79",
   "metadata": {},
   "source": [
    "## DeepL Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c788a18-4c82-44b7-8959-9f8c4e6a9b7d",
   "metadata": {},
   "source": [
    "### Generate Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40d326bf-1199-4933-af3e-fa8386e105b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa55224341d43549be5c52105d863b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:19<00:00,  1.40s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe8174311db4611a12813bb19a8f24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:20<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "deepL_jpen_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/deepL/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "deepL_jpen_comet = evaluate_translation_comet(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/deepL/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "deepL_enjp_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/deepL/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')\n",
    "deepL_enjp_comet = evaluate_translation_comet(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/deepL/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9284ba-7c20-47a5-ac6f-e347b38450be",
   "metadata": {},
   "source": [
    "### Make Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f653d0d3-621c-4f13-a055-43971eee1f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def generate_translation_quality_table(enjp_bleu_chrf, jpen_bleu_chrf, enjp_comet, jpen_comet):\n",
    "    # Define the headers for the table\n",
    "    headers = [\"Segment\", \"EN-JP BLEU\", \"EN-JP CHRF\", \"JP-EN BLEU\", \"JP-EN CHRF\", \"EN-JP COMET\", \"JP-EN COMET\"]\n",
    "\n",
    "    # Define default values for missing dictionaries\n",
    "    default_bleu_chrf = {\"whole_dataset\": (0, 0), \"first_third\": (0, 0), \"second_third\": (0, 0), \"third_third\": (0, 0)}\n",
    "    default_comet = {\"whole_dataset\": 0, \"first_third\": 0, \"second_third\": 0, \"third_third\": 0}\n",
    "\n",
    "    # Use default values if any of the dictionaries are None\n",
    "    enjp_bleu_chrf = enjp_bleu_chrf if enjp_bleu_chrf is not None else default_bleu_chrf\n",
    "    jpen_bleu_chrf = jpen_bleu_chrf if jpen_bleu_chrf is not None else default_bleu_chrf\n",
    "    enjp_comet = enjp_comet if enjp_comet is not None else default_comet\n",
    "    jpen_comet = jpen_comet if jpen_comet is not None else default_comet\n",
    "\n",
    "    # Extract the segments (keys) from one of the dictionaries (they should all have the same keys)\n",
    "    segments = enjp_bleu_chrf.keys()\n",
    "\n",
    "    # Prepare the rows for the table\n",
    "    rows = []\n",
    "    for segment in segments:\n",
    "        enjp_bleu, enjp_chrf = enjp_bleu_chrf[segment]\n",
    "        jpen_bleu, jpen_chrf = jpen_bleu_chrf[segment]\n",
    "        enjp_comet_score = enjp_comet[segment]\n",
    "        jpen_comet_score = jpen_comet[segment]\n",
    "        rows.append([segment, enjp_bleu, enjp_chrf, jpen_bleu, jpen_chrf, enjp_comet_score, jpen_comet_score])\n",
    "\n",
    "    # Generate the table using tabulate\n",
    "    table = tabulate(rows, headers=headers, tablefmt=\"grid\")\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44a057b2-143e-47e7-b752-facc9618d5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| Segment       |   EN-JP BLEU |   EN-JP CHRF |   JP-EN BLEU |   JP-EN CHRF |   EN-JP COMET |   JP-EN COMET |\n",
      "+===============+==============+==============+==============+==============+===============+===============+\n",
      "| whole_dataset |  0.000101534 |      35.0454 |      17.4314 |      48.864  |      0.657306 |      0.772182 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| first_third   |  2.38586e-05 |      32.9298 |      19.5677 |      44.3947 |      0.668454 |      0.758157 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| second_third  |  0.0154445   |      32.9994 |      14.0065 |      42.6844 |      0.688727 |      0.748618 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| third_third   |  1.91163e-08 |      37.4417 |      17.6079 |      55.4434 |      0.614737 |      0.809771 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(generate_translation_quality_table(deepL_enjp_chrf_bleu, deepL_jpen_chrf_bleu, deepL_enjp_comet, deepL_jpen_comet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeba2ed-65ad-4bb2-bafb-372a20a0e50b",
   "metadata": {},
   "source": [
    "## ALMA-R Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd597ef-b9d4-4379-9333-de33a008de6c",
   "metadata": {},
   "source": [
    "### Generate Benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c35702db-430e-4567-a1e2-fe84c37737a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45316b033a414c53a99efa7fff79e375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:40<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "almar_jpen_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/ALMA-R/out3.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "almar_jpen_comet = evaluate_translation_comet(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/ALMA-R/out3.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "# NOTE: enjp values N/A on the 7B param variant of ALMA-R, outputs german "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b41786-e460-497c-924a-b4019567533a",
   "metadata": {},
   "source": [
    "### Make Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07f45765-dabf-4888-89b7-c20a21841720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| Segment       |   EN-JP BLEU |   EN-JP CHRF |   JP-EN BLEU |   JP-EN CHRF |   EN-JP COMET |   JP-EN COMET |\n",
      "+===============+==============+==============+==============+==============+===============+===============+\n",
      "| whole_dataset |            0 |            0 |      9.4352  |      39.0808 |             0 |      0.741773 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| first_third   |            0 |            0 |      9.40995 |      32.6018 |             0 |      0.731449 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| second_third  |            0 |            0 |      9.04974 |      39.2609 |             0 |      0.720376 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| third_third   |            0 |            0 |      9.69305 |      43.7714 |             0 |      0.773494 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(generate_translation_quality_table(None, almar_jpen_chrf_bleu, None, almar_jpen_comet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e47cd3-c35b-4c26-bbcf-fcc0076baab8",
   "metadata": {},
   "source": [
    "## M4T Model Evaluations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73b0ef-32a4-4863-9d32-d73032d8621d",
   "metadata": {},
   "source": [
    "### M4TV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e798e2c-3954-46b5-a846-586261876286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3807ea1c3a44e7a72c02f7e787d91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:16<00:00,  1.34s/it]\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea355475c754310971ccbfb5fc24884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:25<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "m4tV1_jpen_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/m4tv1/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "m4tV1_jpen_comet = evaluate_translation_comet(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/m4tv1/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "m4tV1_enjp_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/m4tv1/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')\n",
    "m4tV1_enjp_comet = evaluate_translation_comet(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/m4tv1/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff76626b-1cbd-48de-8c0e-822170fc1b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| Segment       |   EN-JP BLEU |   EN-JP CHRF |   JP-EN BLEU |   JP-EN CHRF |   EN-JP COMET |   JP-EN COMET |\n",
      "+===============+==============+==============+==============+==============+===============+===============+\n",
      "| whole_dataset |   0.117905   |      20.8832 |     11.4358  |      40.4847 |      0.629256 |      0.696393 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| first_third   |   0.0798192  |      13.003  |      8.1234  |      29.5013 |      0.62253  |      0.620249 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| second_third  |   0.515763   |      22.0577 |      9.04282 |      34.6015 |      0.659661 |      0.672046 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| third_third   |   0.00140051 |      25.4382 |     15.527   |      51.6501 |      0.605578 |      0.796883 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(generate_translation_quality_table(m4tV1_enjp_chrf_bleu, m4tV1_jpen_chrf_bleu, m4tV1_enjp_comet, m4tV1_jpen_comet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edb17b-d38f-4a24-94bd-ee20cc61d24f",
   "metadata": {},
   "source": [
    "### M4Tv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e87a010-4da2-47db-bee4-ca404cb3bf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc47b491f5a4421adc0afa765dd5534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:18<00:00,  1.38s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394f4a670c1248679cc121449d6da235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:33<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "m4tV2_jpen_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/m4tv2/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "m4tV2_jpen_comet = evaluate_translation_comet(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/m4tv2/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "m4tV2_enjp_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/m4tv2/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')\n",
    "m4tV2_enjp_comet = evaluate_translation_comet(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/m4tv2/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65a81da9-1e77-4e73-a086-2fd2cd673d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| Segment       |   EN-JP BLEU |   EN-JP CHRF |   JP-EN BLEU |   JP-EN CHRF |   EN-JP COMET |   JP-EN COMET |\n",
      "+===============+==============+==============+==============+==============+===============+===============+\n",
      "| whole_dataset |   0.141062   |      19.4128 |      8.20461 |      35.622  |      0.616959 |      0.643702 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| first_third   |   0.205474   |      11.9155 |      5.63931 |      25.3349 |      0.606557 |      0.562447 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| second_third  |   0.521246   |      19.9775 |      5.32697 |      29.2089 |      0.64763  |      0.611855 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| third_third   |   0.00333794 |      24.025  |     11.9595  |      46.6148 |      0.59669  |      0.756806 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(generate_translation_quality_table(m4tV2_enjp_chrf_bleu, m4tV2_jpen_chrf_bleu, m4tV2_enjp_comet, m4tV2_jpen_comet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3e9ee-1526-4649-aeeb-eac2b0ee8e1d",
   "metadata": {},
   "source": [
    "## Google Translate Evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "793a09ff-99be-4bd2-9797-5409df02fb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b082f2cc8c8c455d987b20c9206415cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:18<00:00,  1.37s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3789cd0d904f9ba88a75caa80ecc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.2.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:188: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|██| 57/57 [01:26<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "gTranslate_jpen_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/google_translate/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "gTranslate_jpen_comet = evaluate_translation_comet(input_file='model_outputs/test/jp_to_en/in.txt', translated_file='model_outputs/test/jp_to_en/google_translate/out.txt', reference_file='model_outputs/test/jp_to_en/out.txt')\n",
    "gTranslate_enjp_chrf_bleu = evaluate_translation_bleu(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/google_translate/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')\n",
    "gTranslate_enjp_comet = evaluate_translation_comet(input_file='model_outputs/test/en_to_jp/in.txt', translated_file='model_outputs/test/en_to_jp/google_translate/out.txt', reference_file='model_outputs/test/en_to_jp/out.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67d8fe40-45ed-4633-a8d3-e1886fd31070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| Segment       |   EN-JP BLEU |   EN-JP CHRF |   JP-EN BLEU |   JP-EN CHRF |   EN-JP COMET |   JP-EN COMET |\n",
      "+===============+==============+==============+==============+==============+===============+===============+\n",
      "| whole_dataset |  0.0258947   |      34.031  |      19.6339 |      51.888  |      0.65223  |      0.793666 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| first_third   |  0.736565    |      26.5313 |      23.5871 |      49.3255 |      0.63421  |      0.784536 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| second_third  |  0.00402613  |      34.6235 |      17.0647 |      47.6336 |      0.695957 |      0.780795 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n",
      "| third_third   |  9.03664e-06 |      38.7212 |      17.1729 |      56.1235 |      0.626522 |      0.815668 |\n",
      "+---------------+--------------+--------------+--------------+--------------+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(generate_translation_quality_table(gTranslate_enjp_chrf_bleu, gTranslate_jpen_chrf_bleu, gTranslate_enjp_comet, gTranslate_jpen_comet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
