{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6a7161-2d36-483c-877b-35d121948156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def m4t_translate_and_save(df, translator, src_lang, tgt_lang, output_file):\n",
    "    # src_lang, tgt_lang = 'eng' or 'jpn'\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                index_row = 'Japanese' if src_lang == 'jpn' else 'English'\n",
    "                source_text = str(row[index_row])\n",
    "\n",
    "                # Translate the text using the local translator\n",
    "                translated_text, _ = translator.predict(\n",
    "                    input=source_text,\n",
    "                    task_str=\"T2TT\",\n",
    "                    tgt_lang=(tgt_lang),\n",
    "                    src_lang=src_lang,\n",
    "                    unit_generation_opts=None, \n",
    "                )\n",
    "                \n",
    "                # Write the translated text to the output file\n",
    "                file.write(str(translated_text[0]) + '\\n\\n')\n",
    "                #print(\"SRC TEXT:\", source_text, \"\\n\", \"OUTPUT TEXT:\", str(translated_text[0]), \"\\n\") # TODO remove \n",
    "\n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except IOError as e :\n",
    "        print(f\"Error writing to file: {output_file}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145f9ed0-b0a9-48ae-a8e9-c04cc6a015b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "def reassemble_test_dataset(json_path, engl_paths, japn_paths):\n",
    "    \"\"\"\n",
    "    Reassembles the dataset from the original files using indices stored in a JSON file.\n",
    "\n",
    "    Args:\n",
    "    json_path (str): Path to the JSON file containing the indices.\n",
    "    engl_paths (list): List of paths to the English files in the order they were originally concatenated.\n",
    "    japn_paths (list): List of paths to the Japanese files in the order they were originally concatenated.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The reassembled DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the indices from the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        indices = json.load(json_file)\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Extract and concatenate the subsets using the indices\n",
    "    for engl_path, japn_path, index_key in zip(engl_paths, japn_paths, indices):\n",
    "        with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "            engl_lines = engl_file.readlines()\n",
    "            japn_lines = japn_file.readlines()\n",
    "        \n",
    "        # Extract the subset of lines using the index\n",
    "        start_index = indices[index_key]\n",
    "        engl_subset = engl_lines[start_index:start_index + 300]\n",
    "        japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "        # Create a new DataFrame with the extracted lines\n",
    "        new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Concatenate the new DataFrame with the existing one\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e81b6ae-6b14-4d69-90a6-c907fd6175b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = './model_outputs/test/en_to_jp/index.json'\n",
    "engl_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.en',\n",
    "    './datasets/public/pheMT_final/tok.en',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en'\n",
    "]\n",
    "japn_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja',\n",
    "    './datasets/public/pheMT_final/tok.ja',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja'\n",
    "]\n",
    "kftt_phemt_aspec = reassemble_test_dataset(json_path, engl_paths, japn_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a821372f-3e5e-40ba-958c-72b465b959ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of seamlessM4T_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_36langs. Set `force` to `True` to download again.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Using the cached checkpoint of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_v2. Set `force` to `True` to download again.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. Translations saved to: model_outputs/test/en_to_jp/m4tv1/out.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     20\u001b[0m translator_v2 \u001b[38;5;241m=\u001b[39m Translator(\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseamlessM4T_v2_large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocoder_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     24\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m m4t_translate_and_save(kftt_phemt_aspec, translator_v1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/en_to_jp/m4tv1/out.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mm4t_translate_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkftt_phemt_aspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranslator_v1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjpn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meng\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_outputs/test/jp_to_en/m4tv1/out.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m m4t_translate_and_save(kftt_phemt_aspec, translator_v2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/en_to_jp/m4tv2/out.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m m4t_translate_and_save(kftt_phemt_aspec, translator_v2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/test/jp_to_en/m4tv2/out.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m, in \u001b[0;36mm4t_translate_and_save\u001b[0;34m(df, translator, src_lang, tgt_lang, output_file)\u001b[0m\n\u001b[1;32m     10\u001b[0m source_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[index_row])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Translate the text using the local translator\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m translated_text, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT2TT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit_generation_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Write the translated text to the output file\u001b[39;00m\n\u001b[1;32m     22\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(translated_text[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/seamless_communication/inference/translator.py:319\u001b[0m, in \u001b[0;36mTranslator.predict\u001b[0;34m(self, input, task_str, tgt_lang, src_lang, text_generation_opts, unit_generation_opts, spkr, sample_rate, unit_generation_ngram_filtering, duration_factor, prosody_encoder_input, src_text)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unit_generation_opts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     unit_generation_opts \u001b[38;5;241m=\u001b[39m SequenceGeneratorOptions(\n\u001b[1;32m    316\u001b[0m         beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, soft_max_seq_len\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    317\u001b[0m     )\n\u001b[0;32m--> 319\u001b[0m texts, units \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munit_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_modality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_modality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_generation_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit_generation_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43munit_generation_ngram_filtering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit_generation_ngram_filtering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduration_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprosody_encoder_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprosody_encoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_mintox \u001b[38;5;129;01mand\u001b[39;00m task_str \u001b[38;5;241m!=\u001b[39m Task\u001b[38;5;241m.\u001b[39mASR\u001b[38;5;241m.\u001b[39mname:\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mSPEECH:\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/seamless_communication/inference/translator.py:188\u001b[0m, in \u001b[0;36mTranslator.get_prediction\u001b[0;34m(cls, model, text_tokenizer, unit_tokenizer, seqs, padding_mask, input_modality, output_modality, tgt_lang, text_generation_opts, unit_generation_opts, unit_generation_ngram_filtering, duration_factor, prosody_encoder_input)\u001b[0m\n\u001b[1;32m    177\u001b[0m     unit_generation_opts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    179\u001b[0m generator \u001b[38;5;241m=\u001b[39m UnitYGenerator(\n\u001b[1;32m    180\u001b[0m     model,\n\u001b[1;32m    181\u001b[0m     text_tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     unit_opts\u001b[38;5;241m=\u001b[39munit_generation_opts,\n\u001b[1;32m    186\u001b[0m )\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_modality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_modality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngram_filtering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit_generation_ngram_filtering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduration_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprosody_encoder_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprosody_encoder_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/seamless_communication/inference/generator.py:269\u001b[0m, in \u001b[0;36mUnitYGenerator.__call__\u001b[0;34m(self, source_seqs, source_padding_mask, input_modality, output_modality, ngram_filtering, duration_factor, prosody_encoder_input)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt2t_converter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set `use_text_encoder` to `True` in your model config to encode text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         )\n\u001b[0;32m--> 269\u001b[0m     texts, text_gen_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt2t_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_padding_mask\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported input_modality: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_modality\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/text.py:152\u001b[0m, in \u001b[0;36mSequenceToTextConverter.batch_convert\u001b[0;34m(self, source_seqs, source_padding_mask)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(source_seqs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`source_seqs` must contain at least one element, but is empty instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/text.py:91\u001b[0m, in \u001b[0;36mSequenceToTextConverterBase._do_convert\u001b[0;34m(self, source_seqs, source_padding_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# (S) -> (N, S)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m target_prefix_seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_prefix_seq\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m generator_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_prefix_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m texts: List[StringLike] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, hypotheses \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generator_output\u001b[38;5;241m.\u001b[39mhypotheses):\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/beam_search.py:297\u001b[0m, in \u001b[0;36mBeamSearchSeq2SeqGenerator.__call__\u001b[0;34m(self, source_seqs, source_padding_mask, prompt_seqs, prompt_padding_mask)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`min_gen_len` must be less than or equal to `max_gen_len` (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_gen_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_gen_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead. Adjust your `max_gen_len` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m op \u001b[38;5;241m=\u001b[39m _BeamSearchSeq2SeqGeneratorOp(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    279\u001b[0m     encoder_output,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_hooks,\n\u001b[1;32m    295\u001b[0m )\n\u001b[0;32m--> 297\u001b[0m hypotheses \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Seq2SeqGeneratorOutput(hypotheses, encoder_output, encoder_padding_mask)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/beam_search.py:532\u001b[0m, in \u001b[0;36m_BeamSearchSequenceGeneratorOpBase.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_state()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_nr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_prompt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len):\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Sort the hypotheses by their scores before returning.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/beam_search.py:584\u001b[0m, in \u001b[0;36m_BeamSearchSequenceGeneratorOpBase._step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# Generate the next step output.\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_nr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_nr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_bag\u001b[38;5;241m.\u001b[39mincrement_step_nr()\n\u001b[1;32m    588\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/generation/beam_search.py:917\u001b[0m, in \u001b[0;36m_BeamSearchSeq2SeqGeneratorOp._decode\u001b[0;34m(self, seqs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, seqs: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SequenceModelOutput:\n\u001b[0;32m--> 917\u001b[0m     decoder_output, decoder_padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We never use PAD in incremental decoding.\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_bag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mproject(decoder_output, decoder_padding_mask)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/seamless_communication/models/unity/model.py:246\u001b[0m, in \u001b[0;36mUnitYX2TModel.decode\u001b[0;34m(self, seqs, padding_mask, encoder_output, encoder_padding_mask, state_bag)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@finaloverride\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     state_bag: Optional[IncrementalStateBag] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    241\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[PaddingMask]]:\n\u001b[1;32m    242\u001b[0m     seqs, padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_frontend(\n\u001b[1;32m    243\u001b[0m         seqs, padding_mask, state_bag\u001b[38;5;241m=\u001b[39mstate_bag\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[no-any-return]\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_bag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/decoder.py:232\u001b[0m, in \u001b[0;36mStandardTransformerDecoder.forward\u001b[0;34m(self, seqs, padding_mask, encoder_output, encoder_padding_mask, state_bag)\u001b[0m\n\u001b[1;32m    227\u001b[0m     self_attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_mask_factory(\n\u001b[1;32m    228\u001b[0m         seqs, keys\u001b[38;5;241m=\u001b[39mseqs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, state_bag\u001b[38;5;241m=\u001b[39mstate_bag\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mdrop_iter()):\n\u001b[0;32m--> 232\u001b[0m     seqs, padding_mask \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_bag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_output_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hook(layer_idx, seqs, padding_mask, num_layers):\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/decoder_layer.py:232\u001b[0m, in \u001b[0;36mStandardTransformerDecoderLayer.forward\u001b[0;34m(self, seqs, padding_mask, self_attn_mask, encoder_output, encoder_padding_mask, state_bag)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;129m@finaloverride\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     state_bag: Optional[IncrementalStateBag] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    231\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Optional[PaddingMask]]:\n\u001b[0;32m--> 232\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_self_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_encoder_decoder_attn(\n\u001b[1;32m    235\u001b[0m         seqs, padding_mask, encoder_output, encoder_padding_mask, state_bag\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    238\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_ffn(seqs)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/decoder_layer.py:254\u001b[0m, in \u001b[0;36mStandardTransformerDecoderLayer._forward_self_attn\u001b[0;34m(self, seqs, padding_mask, self_attn_mask, state_bag)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_order \u001b[38;5;241m!=\u001b[39m TransformerNormOrder\u001b[38;5;241m.\u001b[39mPOST:\n\u001b[1;32m    252\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(seqs)\n\u001b[0;32m--> 254\u001b[0m seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_bag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_norm(seqs)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/multihead_attention.py:405\u001b[0m, in \u001b[0;36mStandardMultiheadAttention.forward\u001b[0;34m(self, seqs, padding_mask, keys, key_padding_mask, values, attn_mask, state_bag)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`key_padding_mask` must be `None` during incremental decoding.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# k: (N, S_step, M) -> (N, H_kv, S_step, K_h)\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# v: (N, S_step, M) -> (N, H_kv, S_step, V_h)\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_project_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_bag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m state \u001b[38;5;241m=\u001b[39m state_bag\u001b[38;5;241m.\u001b[39mget_state(\u001b[38;5;28mself\u001b[39m, AttentionState)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/transformer/multihead_attention.py:505\u001b[0m, in \u001b[0;36mStandardMultiheadAttention._project_kv\u001b[0;34m(self, keys, key_padding_mask, values, state_bag)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_project_kv\u001b[39m(\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    499\u001b[0m     keys: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# (N, S, K) -> (N, S, K_proj)\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# (N, S, V) -> (N, S, V_proj)\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(values)\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/fairseq2/nn/projection.py:130\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@finaloverride\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from seamless_communication.inference import Translator, SequenceGeneratorOptions\n",
    "from fairseq2.generation import NGramRepeatBlockProcessor\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "translator_v1 = Translator(\n",
    "    \"seamlessM4T_large\",\n",
    "    \"vocoder_36langs\",\n",
    "    device=torch.device(\"cpu\"),\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "translator_v2 = Translator(\n",
    "    \"seamlessM4T_v2_large\",\n",
    "    \"vocoder_v2\",\n",
    "    device=torch.device(\"cpu\"),\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "#m4t_translate_and_save(kftt_phemt_aspec, translator_v1, 'eng', 'jpn', 'model_outputs/test/en_to_jp/m4tv1/out.txt') DONE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce1e83f-58d0-4c02-b95d-879bb4befca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. Translations saved to: model_outputs/test/jp_to_en/m4tv1/out.txt\n",
      "Translation completed. Translations saved to: model_outputs/test/en_to_jp/m4tv2/out.txt\n",
      "Translation completed. Translations saved to: model_outputs/test/jp_to_en/m4tv2/out.txt\n"
     ]
    }
   ],
   "source": [
    "#m4t_translate_and_save(kftt_phemt_aspec, translator_v1, 'jpn', 'eng', 'model_outputs/test/jp_to_en/m4tv1/out.txt') # DONE \n",
    "#m4t_translate_and_save(kftt_phemt_aspec, translator_v2, 'eng', 'jpn', 'model_outputs/test/en_to_jp/m4tv2/out.txt') # DONE \n",
    "#m4t_translate_and_save(kftt_phemt_aspec, translator_v2, 'jpn', 'eng', 'model_outputs/test/jp_to_en/m4tv2/out.txt') # DONE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
