{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6a7161-2d36-483c-877b-35d121948156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def m4t_translate_and_save(df, translator, src_lang, tgt_lang, output_file):\n",
    "    # src_lang, tgt_lang = 'eng' or 'jpn'\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            count = 0 \n",
    "            for _, row in df.iterrows():\n",
    "                index_row = 'Japanese' if src_lang == 'jpn' else 'English'\n",
    "                source_text = str(row[index_row])\n",
    "\n",
    "                # Translate the text using the local translator\n",
    "                translated_text, _ = translator.predict(\n",
    "                    input=source_text,\n",
    "                    task_str=\"T2TT\",\n",
    "                    tgt_lang=(tgt_lang),\n",
    "                    src_lang=src_lang,\n",
    "                    unit_generation_opts=None, \n",
    "                )\n",
    "                \n",
    "                # Write the translated text to the output file\n",
    "                file.write(str(translated_text[0]) + '\\n\\n')\n",
    "                #print(\"SRC TEXT:\", source_text, \"\\n\", \"OUTPUT TEXT:\", str(translated_text[0]), \"\\n\") # TODO remove \n",
    "                count +=1 \n",
    "                if count > 2:\n",
    "                    break \n",
    "\n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except IOError as e :\n",
    "        print(f\"Error writing to file: {output_file}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145f9ed0-b0a9-48ae-a8e9-c04cc6a015b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "def reassemble_test_dataset(json_path, engl_paths, japn_paths):\n",
    "    \"\"\"\n",
    "    Reassembles the dataset from the original files using indices stored in a JSON file.\n",
    "\n",
    "    Args:\n",
    "    json_path (str): Path to the JSON file containing the indices.\n",
    "    engl_paths (list): List of paths to the English files in the order they were originally concatenated.\n",
    "    japn_paths (list): List of paths to the Japanese files in the order they were originally concatenated.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The reassembled DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the indices from the JSON file\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        indices = json.load(json_file)\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Extract and concatenate the subsets using the indices\n",
    "    for engl_path, japn_path, index_key in zip(engl_paths, japn_paths, indices):\n",
    "        with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "            engl_lines = engl_file.readlines()\n",
    "            japn_lines = japn_file.readlines()\n",
    "        \n",
    "        # Extract the subset of lines using the index\n",
    "        start_index = indices[index_key]\n",
    "        engl_subset = engl_lines[start_index:start_index + 300]\n",
    "        japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "        # Create a new DataFrame with the extracted lines\n",
    "        new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Concatenate the new DataFrame with the existing one\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e81b6ae-6b14-4d69-90a6-c907fd6175b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = './model_outputs/test/en_to_jp/index.json'\n",
    "engl_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.en',\n",
    "    './datasets/public/pheMT_final/tok.en',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en'\n",
    "]\n",
    "japn_paths = [\n",
    "    './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja',\n",
    "    './datasets/public/pheMT_final/tok.ja',\n",
    "    './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja'\n",
    "]\n",
    "kftt_phemt_aspec = reassemble_test_dataset(json_path, engl_paths, japn_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a821372f-3e5e-40ba-958c-72b465b959ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "Is MPS (Metal Performance Shader) built? True\n",
      "Is MPS available? True\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of seamlessM4T_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_36langs. Set `force` to `True` to download again.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Using the cached checkpoint of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached tokenizer of seamlessM4T_v2_large. Set `force` to `True` to download again.\n",
      "Using the cached checkpoint of vocoder_v2. Set `force` to `True` to download again.\n",
      "/Users/thomaspett/Desktop/projects/MT_senior_thesis_repo/env/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed. Translations saved to: model_outputs/test/en_to_jp/m4tv1/out.txt\n",
      "Translation completed. Translations saved to: model_outputs/test/jp_to_en/m4tv1/out.txt\n",
      "Translation completed. Translations saved to: model_outputs/test/en_to_jp/m4tv2/out.txt\n",
      "Translation completed. Translations saved to: model_outputs/test/jp_to_en/m4tv2/out.txt\n"
     ]
    }
   ],
   "source": [
    "from seamless_communication.inference import Translator, SequenceGeneratorOptions\n",
    "from fairseq2.generation import NGramRepeatBlockProcessor\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "print(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n",
    "print(f\"Is MPS available? {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "translator_v1 = Translator(\n",
    "    \"seamlessM4T_large\",\n",
    "    \"vocoder_36langs\",\n",
    "    device=torch.device(\"cpu\"),\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "translator_v2 = Translator(\n",
    "    \"seamlessM4T_v2_large\",\n",
    "    \"vocoder_v2\",\n",
    "    device=torch.device(\"cpu\"),\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "m4t_translate_and_save(kftt_phemt_aspec, translator_v1, 'eng', 'jpn', 'model_outputs/test/en_to_jp/m4tv1/out.txt')\n",
    "m4t_translate_and_save(kftt_phemt_aspec, translator_v1, 'jpn', 'eng', 'model_outputs/test/jp_to_en/m4tv1/out.txt')\n",
    "m4t_translate_and_save(kftt_phemt_aspec, translator_v2, 'eng', 'jpn', 'model_outputs/test/en_to_jp/m4tv2/out.txt')\n",
    "m4t_translate_and_save(kftt_phemt_aspec, translator_v2, 'jpn', 'eng', 'model_outputs/test/jp_to_en/m4tv2/out.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
