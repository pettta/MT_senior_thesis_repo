{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d0b669-c8a8-4d2c-9eca-96ffa289294d",
   "metadata": {},
   "source": [
    "# Prepare Test set to be benchmarked across different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2efc4e43-56f1-4dc7-a92d-4fa9c3a82ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepl\n",
      "  Downloading deepl-1.17.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from deepl) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thomaspett/Desktop/pytorch-test/env/lib/python3.10/site-packages (from requests<3,>=2->deepl) (2023.11.17)\n",
      "Downloading deepl-1.17.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: deepl\n",
      "Successfully installed deepl-1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae5746-7f4c-48f9-9ba5-9104793eec2e",
   "metadata": {},
   "source": [
    "## Preprocess ASPEC to same format as other benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea0b4367-7d83-451c-9af5-5e3735ef0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_aspec_data(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name of the input file (without extension)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Create the output file paths in the same directory as the input file\n",
    "    engl_output_path = os.path.join(input_dir, f\"{base_name}.raw.en\")\n",
    "    japn_output_path = os.path.join(input_dir, f\"{base_name}.raw.ja\")\n",
    "\n",
    "    # Open the input file for reading\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Open the output files for writing\n",
    "        with open(engl_output_path, 'w', encoding='utf-8') as engl_file, open(japn_output_path, 'w', encoding='utf-8') as japn_file:\n",
    "            # Iterate over each line in the input file\n",
    "            for line in file:\n",
    "                # Split the line into sections using the delimiter \" ||| \"\n",
    "                sections = line.strip().split(' ||| ')\n",
    "\n",
    "                # Extract the Japanese and English sections (last two sections)\n",
    "                japanese_text = sections[-2]\n",
    "                english_text = sections[-1]\n",
    "\n",
    "                # Write the Japanese text to the Japanese output file\n",
    "                japn_file.write(japanese_text + '\\n')\n",
    "\n",
    "                # Write the English text to the English output file\n",
    "                engl_file.write(english_text + '\\n')\n",
    "\n",
    "    print(f\"Data split completed. Output files: {engl_output_path} and {japn_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73d5e32-48aa-42d5-9b22-ffacd573f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en and ./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja\n",
      "Data split completed. Output files: ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.en and ./datasets/private/ASPEC/ASPEC-JE/test/test.raw.ja\n"
     ]
    }
   ],
   "source": [
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.txt')\n",
    "split_aspec_data('./datasets/private/ASPEC/ASPEC-JE/test/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53975d6-75d7-40ff-92f9-4b30376aee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def tokenize_file(file_path):\n",
    "    # Get the directory of the input file\n",
    "    input_dir = os.path.dirname(file_path)\n",
    "\n",
    "    # Get the base name and extension of the input file\n",
    "    base_name, ext = os.path.splitext(os.path.basename(file_path))\n",
    "\n",
    "    # Create the output file path in the same directory as the input file\n",
    "    output_path = os.path.join(input_dir, f\"{base_name}.tok{ext}\")\n",
    "\n",
    "    # Run KyTea on the input file and generate the tokenized output\n",
    "    subprocess.run(['kytea', '-notags', '-wsconst', 'D', '&lt', '-out', 'tok', '<', file_path, '>', output_path], check=True)\n",
    "\n",
    "    print(f\"Tokenization completed. Output file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac01dbdc-aeba-4b5b-93d4-e5a90786bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this function was really slow --> seconds become minutes, just did it in terminal instead kept it fast \n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.en')\n",
    "#tokenize_file('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.raw.ja')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a2ab0-fa39-40bd-b944-cebda16c03a3",
   "metadata": {},
   "source": [
    "## Create massive dataframe with testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b518ecfb-ec7b-454a-850e-cb93a93e9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def create_parallel_dataframe(engl_path, japn_path, existing_df=None):\n",
    "    # Read the English and Japanese files\n",
    "    with open(engl_path, 'r', encoding='utf-8') as engl_file, open(japn_path, 'r', encoding='utf-8') as japn_file:\n",
    "        engl_lines = engl_file.readlines()\n",
    "        japn_lines = japn_file.readlines()\n",
    "\n",
    "    # Get the total number of lines in each file\n",
    "    total_lines = min(len(engl_lines), len(japn_lines))\n",
    "\n",
    "    # Check if there are at least 300 lines in both files\n",
    "    if total_lines < 300:\n",
    "        raise ValueError(\"Files must contain at least 300 lines.\")\n",
    "\n",
    "    # Generate a random starting index for the interval\n",
    "    start_index = random.randint(0, total_lines - 300)\n",
    "\n",
    "    # Extract the random interval of 300 lines from each file\n",
    "    engl_subset = engl_lines[start_index:start_index + 300]\n",
    "    japn_subset = japn_lines[start_index:start_index + 300]\n",
    "\n",
    "    # Create a new DataFrame with the extracted lines\n",
    "    new_data = {'English': engl_subset, 'Japanese': japn_subset}\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # If an existing DataFrame is provided, concatenate the new data with it\n",
    "    if existing_df is not None:\n",
    "        df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        df = new_df\n",
    "\n",
    "    return df, start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56755041-cf4e-4335-b576-e9139b96d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               English  \\\n",
      "0                Volume 3 consists of 102 chapters .\\n   \n",
      "1                 Volume 4 consists of 32 chapters .\\n   \n",
      "2                    Volume 5 might have been lost .\\n   \n",
      "3                             It takes Ruisan form .\\n   \n",
      "4    There exists only one manuscript that has been...   \n",
      "..                                                 ...   \n",
      "895  Capacity   as   human   and  abundant   experi...   \n",
      "896  In  the  future ,   conferences  are  improved...   \n",
      "897  Serveillance   of   11   cases  receiving   vi...   \n",
      "898    Most   of  nursing   persons   are  spouses .\\n   \n",
      "899  No   bedsore   was  observed  in  the   patien...   \n",
      "\n",
      "                                              Japanese  \n",
      "0                                        3 巻 102 話 。\\n  \n",
      "1                                         4 話 32 話 。\\n  \n",
      "2                                           5 巻 紛失 ?\\n  \n",
      "3                                      類纂 形態 を と る 。\\n  \n",
      "4          加賀 国 、 前田 家 伝来 本 （ 前田 育徳 会 蔵 ） が あ る のみ 。\\n  \n",
      "..                                                 ...  \n",
      "895  告知 に 関わ る に は ， 人間 と し て の 幅 の 広 さ ， 体験 の 豊か さ...  \n",
      "896       今後 ， 意志 統一 の 図れ る 様 カンファレンス を 充実 さ せ て い く\\n  \n",
      "897  脳 神経 外科 的 疾患 に よ っ て 入院 し ， 遷延 性 意識 障害 の ため ， ...  \n",
      "898                        介護 者 は 配偶 者 が 最も 多 かっ た 。\\n  \n",
      "899  家庭 内 協力 体制 が あ る 患者 で は じょくそう が な かっ た が ， 協力 ...  \n",
      "\n",
      "[900 rows x 2 columns] 506 765 692\n"
     ]
    }
   ],
   "source": [
    "kftt_out, kftt_start_index = create_parallel_dataframe('./datasets/public/kftt-data-1.0/data/tok/kyoto-test.en', './datasets/public/kftt-data-1.0/data/tok/kyoto-test.ja') \n",
    "kftt_and_phemt, phemt_start_index =  create_parallel_dataframe('./datasets/public/pheMT_final/tok.en', './datasets/public/pheMT_final/tok.ja', kftt_out)\n",
    "kftt_phemt_aspec, aspec_start_index = create_parallel_dataframe('./datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.en', './datasets/private/ASPEC/ASPEC-JE/devtest/devtest.tok.ja', kftt_and_phemt)\n",
    "print(kftt_phemt_aspec, kftt_start_index, phemt_start_index, aspec_start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0fcd1-045b-499f-abae-b69a768c1c1b",
   "metadata": {},
   "source": [
    "# Do some checks on max characters so i dont go over free api limits and go bankrupt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3b4161e-7926-4456-ad1f-b7687bac4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_characters(df):\n",
    "    # Count the total number of characters in the Japanese column\n",
    "    total_japanese_chars = df['Japanese'].str.len().sum()\n",
    "\n",
    "    # Count the total number of characters in the English column\n",
    "    total_english_chars = df['English'].str.len().sum()\n",
    "\n",
    "    # Create a dictionary with the counts\n",
    "    char_counts = {\n",
    "        \"japanese_characters\": total_japanese_chars,\n",
    "        \"english_characters\": total_english_chars\n",
    "    }\n",
    "    print(char_counts) \n",
    "    return char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "835480ea-75ac-44a4-8b2a-519ecdff26a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'japanese_characters': 59923, 'english_characters': 130927}\n",
      "Total characters going to API calls = 190850\n"
     ]
    }
   ],
   "source": [
    "tot_chars = count_total_characters(kftt_phemt_aspec.copy())\n",
    "total = tot_chars['japanese_characters'] + tot_chars['english_characters']\n",
    "print(f\"Total characters going to API calls = {total}\")\n",
    "assert total < 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5278baa-22ba-4cd5-aeda-2c7783650a9a",
   "metadata": {},
   "source": [
    "# API tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e5de-e70b-45d6-84f1-67b1e9d6e1e8",
   "metadata": {},
   "source": [
    "## deepL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c17095ef-8622-4486-af42-94f5ca7902b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "def translate_and_save(df, api_key, output_file):\n",
    "    try:\n",
    "        # Initialize the DeepL translator\n",
    "        translator = deepl.Translator(api_key)\n",
    "\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                japanese_text = row['Japanese']\n",
    "\n",
    "                # Translate the Japanese text to English\n",
    "                result = translator.translate_text(japanese_text, target_lang=\"EN-US\")\n",
    "                translated_text = result.text\n",
    "\n",
    "                # Write the translated text to the output file\n",
    "                file.write(translated_text + '\\n')\n",
    "\n",
    "        print(f\"Translation completed. Translations saved to: {output_file}\")\n",
    "\n",
    "    except deepl.exceptions.AuthorizationException:\n",
    "        print(\"Invalid API key. Please check your API key.\")\n",
    "\n",
    "    except deepl.exceptions.QuotaExceededException:\n",
    "        print(\"Quota exceeded. Please check your DeepL usage limits.\")\n",
    "\n",
    "    except deepl.exceptions.DeepLException as e:\n",
    "        print(f\"An error occurred during translation: {str(e)}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8a8a3c9-df3b-4034-9de9-ff80cb145944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            api_key = file.readline().strip()\n",
    "            return api_key\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except IOError:\n",
    "        print(f\"Error reading file: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "54122365-caad-449a-a71a-6a1924f80549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_column(df, column, output_file):\n",
    "    try:\n",
    "        # Open the output file in write mode\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for _, row in df.iterrows():\n",
    "                japanese_text = row[column]\n",
    "\n",
    "                # Write the Japanese text to the output file\n",
    "                file.write(japanese_text + '\\n')\n",
    "\n",
    "        print(f\"{column} saved to: {output_file}\")\n",
    "\n",
    "    except IOError:\n",
    "        print(f\"Error writing to file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a55544f4-bf31-4d2f-8b55-7455332df680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English saved to: model_outputs/test/jp_to_en/out.txt\n"
     ]
    }
   ],
   "source": [
    "# NOTE: i ran this, its commented out so i dont rerun and go over api limit \n",
    "#api_key = get_api_key('./datasets/private/apikey.txt')\n",
    "#save_column(kftt_phemt_aspec, 'Japanese', 'model_outputs/test/jp_to_en/in.txt')\n",
    "save_column(kftt_phemt_aspec, 'English', 'model_outputs/test/jp_to_en/out.txt')\n",
    "#translated_english = translate_and_save(kftt_phemt_aspec, api_key, 'model_outputs/test/jp_to_en/deepL/out.txt')\n",
    "# print(\"Original Japanese:\", kftt_phemt_aspec['Japanese'].iloc[0])\n",
    "# print(\"Translated English:\", translated_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c93281-e12e-4839-a7b0-83cbcb05eda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
